{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfd2cb92",
   "metadata": {},
   "source": [
    "# Adversarial Example\n",
    "\n",
    "An adversarial example refers to a specially crafted input data point that is intentionally designed to deceive a machine learning model. It is created by making subtle, often imperceptible modifications to the original input data, such as images, audio, or text, with the intention of causing the model to misclassify or produce incorrect outputs.\n",
    "\n",
    "The goal of an adversarial example is to exploit vulnerabilities or weaknesses in the model's decision-making process. By manipulating the input data in a strategic manner, an attacker can cause the model to make mistakes or produce undesired outcomes. These adversarial examples are typically created with the intent to deceive the model, rather than being naturally occurring data points.\n",
    "\n",
    "The concept of adversarial examples highlights the limitations and vulnerabilities of machine learning models. It demonstrates that even advanced models can be susceptible to manipulation and can be easily fooled by carefully crafted inputs. Adversarial examples raise concerns about the robustness, reliability, and security of machine learning systems in real-world applications.\n",
    "\n",
    "Understanding and studying adversarial examples is crucial for developing more robust and resilient machine learning models. Researchers and practitioners aim to develop defense mechanisms and techniques to detect and mitigate the impact of adversarial examples, improving the overall security and trustworthiness of machine learning systems.\n",
    "\n",
    "In summary, an adversarial example is a specially crafted input designed to exploit the vulnerabilities of a machine learning model, causing it to produce incorrect outputs or make mistakes. It serves as a test and a means to understand the limitations of machine learning systems and develop defenses against potential attacks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03858db9",
   "metadata": {},
   "source": [
    "# White-box Adversarial Examples\n",
    "\n",
    "White-box adversarial examples refer to adversarial attacks where the attacker has complete knowledge about the targeted machine learning model. In a white-box setting, the attacker has access to the model's architecture, parameters, and training data, enabling them to analyze and exploit its vulnerabilities effectively.\n",
    "\n",
    "To create a white-box adversarial example, the attacker can employ various optimization algorithms and techniques. They can use gradient information, obtained through backpropagation, to compute the direction and magnitude of perturbations that will maximize the model's prediction error. By iteratively adjusting the input data based on the gradient information, the attacker can craft an adversarial example that leads to a misclassification or an undesired output from the model.\n",
    "\n",
    "The main advantage of white-box attacks is the extensive knowledge available to the attacker. They can thoroughly analyze the model's weaknesses, understand its decision boundaries, and tailor the adversarial example accordingly. This access to detailed information allows for more precise and effective manipulation of the input data.\n",
    "\n",
    "White-box adversarial examples are particularly concerning because they simulate a scenario where the attacker has insider knowledge about the targeted model. This knowledge can be leveraged to design highly targeted attacks that exploit specific vulnerabilities, potentially causing severe consequences in real-world applications.\n",
    "\n",
    "Defending against white-box adversarial examples requires the development of robust and resilient machine learning models. Techniques like adversarial training, defensive distillation, and input preprocessing can enhance the model's ability to withstand such attacks. Additionally, ensuring model transparency, frequent security evaluations, and careful consideration of model architectures can help mitigate the impact of white-box attacks.\n",
    "\n",
    "In summary, white-box adversarial examples are crafted by attackers who possess complete knowledge of the targeted model. They exploit this knowledge to design precise and effective attacks, highlighting vulnerabilities in the model's decision-making process. Defending against white-box attacks requires robust model design, proactive security measures, and ongoing evaluation of the model's vulnerabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee1e33a",
   "metadata": {},
   "source": [
    "# Black-box Adversarial Examples \n",
    "Black-box adversarial examples refer to adversarial attacks where the attacker has limited or no knowledge about the targeted machine learning model. In a black-box setting, the attacker doesn't have access to the model's internal architecture, parameters, or training data. They can only interact with the model by providing inputs and observing its outputs.\n",
    "\n",
    "Creating black-box adversarial examples presents a greater challenge for the attacker compared to white-box attacks. Without detailed knowledge of the model, they need to rely on techniques that explore the model's behavior through input-output observations and feedback.\n",
    "\n",
    "One common approach in black-box attacks is the use of transferability. The attacker trains a surrogate model, either by collecting their own labeled data or using a publicly available dataset. This surrogate model mimics the behavior of the targeted model to some extent. By crafting adversarial examples on the surrogate model, the attacker assumes that these examples will also fool the targeted model due to the transferability of adversarial perturbations.\n",
    "\n",
    "Another technique employed in black-box attacks is the query-based method. The attacker can query the targeted model multiple times, providing carefully selected inputs and observing the corresponding outputs. By analyzing the model's responses and monitoring the changes in predictions, the attacker can gradually refine their understanding of the model's decision boundaries and craft adversarial examples.\n",
    "\n",
    "Black-box attacks are particularly challenging because they simulate scenarios where the attacker has limited information and has to work with restricted access to the targeted model. These attacks closely resemble real-world situations where models are deployed as services, and the internal workings are not exposed.\n",
    "\n",
    "Defending against black-box adversarial examples requires robustness even in the absence of specific knowledge about the attack. Techniques like adversarial training, ensemble methods, and input sanitization can enhance the model's resilience against unseen adversarial inputs. Additionally, monitoring and anomaly detection systems can help identify suspicious patterns and inputs that exhibit adversarial behavior.\n",
    "\n",
    "In summary, black-box adversarial examples are crafted by attackers who have limited or no knowledge about the targeted model. They rely on techniques like transferability and query-based methods to explore the model's behavior and create adversarial examples. Defending against black-box attacks requires robust models, proactive security measures, and techniques that can generalize to unseen adversarial inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1a4e6f",
   "metadata": {},
   "source": [
    "# Risks in Adversarial Example\n",
    "When it comes to applications of classification, such as text or image classification, adversarial examples pose several risks. Here are some of the major risks associated with adversarial examples:\n",
    "\n",
    "1. Misclassification: Adversarial examples can cause misclassification, where a model wrongly predicts the class or label of an input. By making small, imperceptible modifications to the input data, an attacker can trick the model into producing incorrect predictions. This misclassification can lead to severe consequences, especially in critical applications like autonomous vehicles or medical diagnosis.\n",
    "\n",
    "2. Security Breaches: Adversarial examples can be used as a means to exploit vulnerabilities in the classification system. Attackers can craft adversarial inputs specifically designed to bypass security measures, gain unauthorized access, or deceive the system for malicious purposes. This can compromise the integrity and security of sensitive data and systems.\n",
    "\n",
    "3. Privacy Violation: Adversarial examples can potentially reveal sensitive information about the input data or compromise user privacy. By manipulating inputs, attackers might be able to extract sensitive features or fool the system into revealing confidential information. This poses a significant risk in applications that deal with personal data or sensitive information.\n",
    "\n",
    "4. Model Robustness Evaluation: Adversarial examples serve as a critical tool for evaluating the robustness and reliability of classification models. By exposing models to adversarial attacks, researchers and developers can identify vulnerabilities, weaknesses, and limitations in the models' performance. Failure to account for adversarial examples during model evaluation can lead to overestimation of performance and create a false sense of security.\n",
    "\n",
    "5. Transferability: Adversarial examples can exhibit transferability, meaning that an adversarial example designed to fool one model can also fool other models or even different machine learning algorithms. This transferability poses a risk across different systems and implementations, as an attack designed for one model can potentially affect multiple models or classifiers.\n",
    "\n",
    "6. Social Engineering: Adversarial examples can be used as a form of social engineering, manipulating the perception of systems to deceive users or gain their trust. By crafting adversarial inputs, attackers can create a false sense of credibility or authority, leading users to make decisions based on incorrect or manipulated information.\n",
    "\n",
    "7. Ethical Implications: The existence of adversarial examples raises ethical concerns related to the trustworthiness and fairness of classification systems. Adversarial attacks can disproportionately affect certain groups or bias decision-making processes. Addressing these ethical implications and ensuring fairness in classification systems is crucial to maintain public trust and avoid discriminatory outcomes.\n",
    "\n",
    "8. Adversarial Attacks on Training Data: Adversarial examples can also be used to manipulate the training data itself. By introducing subtle perturbations or modifications to the training samples, attackers can influence the learning process and bias the model's behavior. This can lead to biased predictions, compromised model performance, or unintended consequences during deployment.\n",
    "\n",
    "9. Legal and Regulatory Compliance: The presence of adversarial examples can have legal and regulatory implications, particularly in domains where accuracy and reliability are critical. Compliance with regulations, such as data protection or safety standards, may require thorough evaluation and mitigation strategies against adversarial attacks to ensure the integrity and trustworthiness of the classification system.\n",
    "\n",
    "It is important to note that the risks associated with adversarial examples depend on the specific application, the potential impact of misclassification or manipulation, and the threat landscape. Understanding and mitigating these risks is essential for the development and deployment of robust and secure classification systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257d7920",
   "metadata": {},
   "source": [
    "# How to Protect Classification Systems against this Type of Attack\n",
    "To protect classification systems against adversarial attacks, several strategies can be employed. Here are some approaches:\n",
    "\n",
    "1. Adversarial Training: This technique involves augmenting the training data with adversarial examples during the model training phase. By exposing the model to adversarial examples and incorporating them into the training process, the model learns to be more robust and resilient to such attacks. Adversarial training can enhance the model's ability to generalize and classify both clean and adversarial inputs accurately.\n",
    "\n",
    "2. Defensive Distillation: This method involves training a model in two stages. In the first stage, a teacher model is trained on the dataset using standard training techniques. In the second stage, a student model is trained to mimic the behavior of the teacher model. However, during this stage, the training data is modified by adding perturbations. The objective is to make the student model more resistant to adversarial attacks.\n",
    "\n",
    "3. Input Preprocessing: Applying preprocessing techniques to the input data can help detect and mitigate adversarial attacks. Techniques like input normalization, feature scaling, and noise injection can help make the model more robust against perturbations introduced by attackers. Additionally, input validation and anomaly detection methods can be employed to identify potential adversarial inputs and reject or flag them for further analysis.\n",
    "\n",
    "4. Ensemble Methods: Using ensemble methods, which combine the predictions of multiple models, can enhance the system's robustness. By training and combining multiple models with different architectures or initializations, the ensemble can collectively make more accurate predictions and be more resistant to adversarial attacks. Adversarial examples that affect one model in the ensemble are less likely to have the same effect on all models, reducing the overall vulnerability.\n",
    "\n",
    "5. Model Regularization: Applying regularization techniques such as L1 or L2 regularization, dropout, or batch normalization during model training can help reduce the impact of adversarial perturbations. Regularization adds constraints to the model's parameters, making it less sensitive to small changes in the input data. Regularization can improve the model's generalization capability and make it more robust against adversarial attacks.\n",
    "\n",
    "6. Adversarial Detection and Defense Mechanisms: Implementing adversarial detection and defense mechanisms can help identify and mitigate adversarial attacks. These mechanisms can include techniques like outlier detection, anomaly detection, or using specialized detection models to identify potential adversarial inputs. Once identified, appropriate actions can be taken, such as rejecting or flagging the inputs for manual review or applying additional defenses.\n",
    "\n",
    "7. Ongoing Research and Development: Adversarial attacks are continuously evolving, and new defense techniques are being developed to counter them. Staying updated with the latest research and incorporating state-of-the-art defense mechanisms can help protect classification systems against emerging adversarial attack methods.\n",
    "\n",
    "It's important to note that while these techniques can enhance the system's resilience, they may not provide complete protection against all types of adversarial attacks. Adversarial attacks and defense mechanisms are an ongoing cat-and-mouse game, and it's crucial to remain vigilant and adapt to new attack techniques and defense strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea2375a",
   "metadata": {},
   "source": [
    "# Notebook \n",
    "The objective of this notebook is to perform a series of operations using a pre-trained model (ResNet-50) to generate an adversarial image from an input image. Let's go through the steps of the code:\n",
    "\n",
    "1. Import the necessary libraries:\n",
    "2. Load a pre-trained model:\n",
    "3. Set up the model for evaluation:\n",
    "4. Define the loss function:\n",
    "5. Define preprocessing transformations:\n",
    "6. Load and preprocess the image:\n",
    "7. Save the original image:\n",
    "8. Define the target label:\n",
    "9. Set up the image tensor for gradient calculation:\n",
    "10. Make a prediction on the original image:\n",
    "11. Calculate the loss and backpropagate the gradient:\n",
    "12. Calculate the sign gradients:\n",
    "13. Generate the adversarial image:\n",
    "14. Save the adversarial image:\n",
    "15. Make a prediction on the adversarial image:\n",
    "16. Print the original prediction:\n",
    "\n",
    "The code is typically used to demonstrate the process of creating an adversarial image using a pre-trained model and PyTorch. It highlights the steps involved in generating the adversarial image and evaluating its impact on the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "264e5add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the watermark package.\n",
    "# This package is used to record the versions of other packages used in this Jupyter notebook.\n",
    "# https://github.com/rasbt/watermark\n",
    "!pip install -q -U watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3bc9f617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # Importing the PyTorch library for deep learning\n",
    "import torch.nn as nn  # Importing the neural network module of PyTorch\n",
    "import torch.optim as optim  # Importing the optimization module of PyTorch\n",
    "import torchvision.transforms as transforms  # Importing image transformations from torchvision\n",
    "import torchvision.models as models  # Importing pre-trained models from torchvision\n",
    "from PIL import Image  # Importing the Python Imaging Library for image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "174bd5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings  # Import the warnings library for warning control and management\n",
    "\n",
    "# Disable all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "afc9ce2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Fabiano Falcão\n",
      "\n",
      "Website: https://fabianumfalco.github.io/\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.10.6\n",
      "IPython version      : 8.11.0\n",
      "\n",
      "PIL        : 9.0.1\n",
      "torch      : 2.0.0\n",
      "torchvision: 0.15.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the watermark extension to display information about the Python version and installed packages.\n",
    "%reload_ext watermark\n",
    "\n",
    "# Display the versions of Python and installed packages.\n",
    "%watermark -a 'Fabiano Falcão' -ws \"https://fabianumfalco.github.io/\" --python --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "769e8b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set the device to CUDA (GPU) if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n",
    "print(f\"Using device: {device}\")  # Print the device being used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6caac56",
   "metadata": {},
   "source": [
    "# ResNet-50\n",
    "\n",
    "The ResNet-50 is a convolutional neural network (CNN) architecture that was proposed as a solution to the problem of vanishing gradients in deep network training. It was first introduced in the paper \"Deep Residual Learning for Image Recognition\" by Kaiming He et al. in 2015.\n",
    "\n",
    "The ResNet-50 consists of 50 layers, including convolutional, activation, and pooling layers. The main innovation of ResNet-50 is the use of residual connections, which allow the gradient information to be directly propagated through the layers, even in deep networks. These residual connections enable the model to learn richer and deeper representations of the images, thereby improving the accuracy of object recognition.\n",
    "\n",
    "ResNet-50 was trained on a large dataset, such as ImageNet, which contains millions of images across various classes. This allowed the model to learn general features of a wide range of objects and textures. As a result, ResNet-50 has become a benchmark model for image classification tasks, achieving state-of-the-art performance on object recognition benchmarks.\n",
    "\n",
    "Furthermore, due to its deep learning capacity and rich representations, ResNet-50 has also been used as a base for transfer learning, where the learned features can be transferred to related tasks such as object detection and semantic segmentation.\n",
    "\n",
    "In summary, ResNet-50 is a powerful CNN architecture that overcame the limitations of deep networks by introducing residual connections. It excelled in image classification tasks and has served as a foundation for many other computer vision applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d94d15e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pre-trained ResNet-50 model from torchvision\n",
    "# https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html\n",
    "# https://pytorch.org/hub/nvidia_deeplearningexamples_resnet50/\n",
    "model = models.resnet50(pretrained=True)  \n",
    "\n",
    "# Move the model to GPU if available\n",
    "model = model.to(device)\n",
    "\n",
    "# sets the model to evaluation mode. \n",
    "# In this mode, the model behaves differently during inference compared to training. \n",
    "# For example, dropout layers are deactivated, batch normalization layers use the running statistics, \n",
    "#and the gradients are not computed for parameters. Setting the model to evaluation mode is important \n",
    "#to ensure consistent and accurate results during inference.\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46495971",
   "metadata": {},
   "source": [
    "# Cross entropy loss\n",
    "\n",
    "Cross entropy loss, also known as softmax loss or log loss, is a commonly used loss function in machine learning for multi-class classification tasks. It measures the dissimilarity between the predicted probability distribution and the true distribution of the target labels.\n",
    "\n",
    "In the context of classification, the cross entropy loss quantifies the difference between the predicted class probabilities and the actual class labels. It calculates the average negative log likelihood of the predicted probabilities for the true class labels. The goal is to minimize this loss function during the training process to improve the accuracy of the model.\n",
    "\n",
    "The formula for cross entropy loss involves taking the logarithm of the predicted probabilities, multiplying them with the true label's one-hot encoded representation, and summing the values across all classes. The loss is then averaged over the batch or the entire dataset.\n",
    "\n",
    "By optimizing the cross entropy loss, the model learns to assign higher probabilities to the correct class labels and lower probabilities to the incorrect ones. This loss function encourages the model to produce more confident predictions for the correct classes and penalizes uncertain or incorrect predictions.\n",
    "\n",
    "The cross entropy loss is widely used because it is effective for optimizing models in multi-class classification tasks. It provides a gradient signal that guides the model's parameters towards better predictions, facilitating the learning process. Many deep learning frameworks, including PyTorch, provide an implementation of the cross entropy loss for easy integration into the training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ca60b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function for multi-class classification tasks\n",
    "# initializing criterion with nn.CrossEntropyLoss(), we can use it later in the training loop \n",
    "# to compute and backpropagate the loss, updating the model's parameters to improve its performance \n",
    "# in the classification task.\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e8856451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The transforms.Compose() function is used to chain the transformations together into \n",
    "# a single pipeline, allowing for easy and efficient preprocessing of the input images before\n",
    "# feeding them into the model.\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),  # Resize the image to a square of size 256x256 pixels\n",
    "    transforms.CenterCrop(224),  # Crop the center of the image to a size of 224x224 pixels\n",
    "    transforms.ToTensor()  # Convert the image to a tensor representation\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "afe08ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the input image and convert it into a tensor\n",
    "# .unsqueeze(0): This adds an extra dimension to the tensor, making it a 4-dimensional tensor\n",
    "# with a batch size of 1. This is necessary as many deep learning models expect inputs in batch format.\n",
    "image = preprocess(Image.open('dog.jpg')).unsqueeze(0)  \n",
    "\n",
    "# Move the image and model to GPU if available\n",
    "image = image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8938ad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the extra batch dimension from the image tensor\n",
    "image2 = torch.squeeze(image, 0)\n",
    "\n",
    "# Create an instance of the ToPILImage transformation\n",
    "T = transforms.ToPILImage()\n",
    "\n",
    "# Convert the tensor image back to a PIL Image object\n",
    "img = T(image2)\n",
    "\n",
    "# Save the PIL Image as 'dog_original.jpg'\n",
    "img.save('dog_original.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b54ad128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the target label to 3\n",
    "# In the ResNet-50 architecture of torchvision.models, the label 3 corresponds to the specific class \"cat\" \n",
    "# within the ImageNet dataset. \n",
    "# https://deeplearning.cms.waikato.ac.nz/user-guide/class-maps/IMAGENET/\n",
    "target_label = 3 # tiger shark, Galeocerdo cuvieri\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f48e91",
   "metadata": {},
   "source": [
    "# epsilon (ε)\n",
    "\n",
    "In the context of adversarial attacks or perturbation-based techniques, epsilon (ε) is a small positive constant used to control the magnitude of perturbation applied to an image. It determines the maximum allowable change in pixel values for generating adversarial examples while maintaining visual similarity to the original image.\n",
    "\n",
    "The purpose of using epsilon is to strike a balance between the effectiveness of the attack and the perceptibility of the perturbation. By setting an appropriate value for epsilon, one can control the level of distortion introduced to the image to deceive a machine learning model.\n",
    "\n",
    "During adversarial attacks, the original image is perturbed by adding imperceptible perturbations to the pixel values. The magnitude of these perturbations is constrained by epsilon. A smaller epsilon value limits the amount of perturbation, making it less likely to be detected by humans but potentially less effective in fooling the model. On the other hand, a larger epsilon value allows for more significant perturbations, increasing the likelihood of misleading the model but potentially introducing noticeable visual changes.\n",
    "\n",
    "The choice of epsilon depends on factors such as the sensitivity of the targeted model, the specific attack technique being employed, and the desired trade-off between stealthiness and attack success rate. It is often determined through experimentation and fine-tuning to achieve the desired level of adversarial perturbation while minimizing perceptibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fad5eefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the value of epsilon to 0.03\n",
    "epsilon = 0.03\n",
    "\n",
    "# Enable gradient calculation for the image tensor\n",
    "image.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b7585d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform forward pass through the model\n",
    "output = model(image)\n",
    "\n",
    "# Calculate the loss using the criterion of the model's output compared to a target label\n",
    "loss = criterion(output, torch.tensor([target_label]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "95c97faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset gradients to zero\n",
    "model.zero_grad()\n",
    "\n",
    "# Perform backward pass to calculate gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "da9f2349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sign of the gradients of the image\n",
    "# Determine the direction of perturbation that maximizes the loss function.\n",
    "# image.grad represents the gradients of the image with respect to some loss function. Gradients capture \n",
    "# the direction and magnitude of the steepest ascent in the loss landscape.\n",
    "sign_grad = torch.sign(image.grad.data)\n",
    "\n",
    "# In the context of adversarial attacks, the sign of gradients helps determine the direction to perturb \n",
    "# the image in order to create an adversarial example that can mislead the model's predictions.\n",
    "\n",
    "# Generate the adversarial image by adding perturbation based on the sign of gradients\n",
    "adversarial_image = image + epsilon * sign_grad\n",
    "\n",
    "# By multiplying the perturbation (scaled by epsilon) with the sign of the gradients and adding it \n",
    "# to the original image, we introduce a small distortion to the image in the direction indicated \n",
    "# by the sign of the gradients. This distortion is intended to create an adversarial example that \n",
    "# can potentially fool a machine learning model.\n",
    "\n",
    "# The resulting adversarial_image is a tensor with the same shape as the original image, where \n",
    "# each pixel has been modified according to the sign of the gradients. The purpose is to create \n",
    "# a visually similar image that can lead to different predictions or misclassify the input when fed into \n",
    "# a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bd1608b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squeeze the dimensions of adversarial_image\n",
    "adversarial_image2 = torch.squeeze(adversarial_image, 0)\n",
    "\n",
    "# Convert the tensor to a PIL image\n",
    "T = transforms.ToPILImage()\n",
    "img = T(adversarial_image2)\n",
    "\n",
    "# Save the adversarial image to disk\n",
    "img.save('dog_adversarial.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5db91d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original prediction: 207\n",
      "Adversarial prediction: 222\n"
     ]
    }
   ],
   "source": [
    "# Predefined list of ImageNet labels\n",
    "# https://deeplearning.cms.waikato.ac.nz/user-guide/class-maps/IMAGENET/\n",
    "\n",
    "# Print the original prediction from the model\n",
    "# 207 - golden retriever\n",
    "print(\"Original prediction:\", torch.argmax(output).item())\n",
    "\n",
    "# Print the adversarial prediction obtained after applying the perturbation\n",
    "# 222 - kuvasz\n",
    "print(\"Adversarial prediction:\", adversarial_prediction.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
