{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "264e5add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the watermark package.\n",
    "# This package is used to record the versions of other packages used in this Jupyter notebook.\n",
    "# https://github.com/rasbt/watermark\n",
    "!pip install -q -U watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3bc9f617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # Importing the PyTorch library for deep learning\n",
    "import torch.nn as nn  # Importing the neural network module of PyTorch\n",
    "import torch.optim as optim  # Importing the optimization module of PyTorch\n",
    "import torchvision.transforms as transforms  # Importing image transformations from torchvision\n",
    "import torchvision.models as models  # Importing pre-trained models from torchvision\n",
    "from PIL import Image  # Importing the Python Imaging Library for image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c659ed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Disable all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "afc9ce2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Fabiano Falcão\n",
      "\n",
      "Website: https://fabianumfalco.github.io/\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.10.6\n",
      "IPython version      : 8.11.0\n",
      "\n",
      "PIL        : 9.0.1\n",
      "torch      : 2.0.0\n",
      "torchvision: 0.15.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the watermark extension to display information about the Python version and installed packages.\n",
    "%reload_ext watermark\n",
    "\n",
    "# Display the versions of Python and installed packages.\n",
    "%watermark -a 'Fabiano Falcão' -ws \"https://fabianumfalco.github.io/\" --python --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b97465f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set the device to CUDA (GPU) if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n",
    "print(f\"Using device: {device}\")  # Print the device being used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1335a9f",
   "metadata": {},
   "source": [
    "# ResNet-50\n",
    "\n",
    "The ResNet-50 is a convolutional neural network (CNN) architecture that was proposed as a solution to the problem of vanishing gradients in deep network training. It was first introduced in the paper \"Deep Residual Learning for Image Recognition\" by Kaiming He et al. in 2015.\n",
    "\n",
    "The ResNet-50 consists of 50 layers, including convolutional, activation, and pooling layers. The main innovation of ResNet-50 is the use of residual connections, which allow the gradient information to be directly propagated through the layers, even in deep networks. These residual connections enable the model to learn richer and deeper representations of the images, thereby improving the accuracy of object recognition.\n",
    "\n",
    "ResNet-50 was trained on a large dataset, such as ImageNet, which contains millions of images across various classes. This allowed the model to learn general features of a wide range of objects and textures. As a result, ResNet-50 has become a benchmark model for image classification tasks, achieving state-of-the-art performance on object recognition benchmarks.\n",
    "\n",
    "Furthermore, due to its deep learning capacity and rich representations, ResNet-50 has also been used as a base for transfer learning, where the learned features can be transferred to related tasks such as object detection and semantic segmentation.\n",
    "\n",
    "In summary, ResNet-50 is a powerful CNN architecture that overcame the limitations of deep networks by introducing residual connections. It excelled in image classification tasks and has served as a foundation for many other computer vision applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d94d15e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pre-trained ResNet-50 model from torchvision\n",
    "# https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html\n",
    "# https://pytorch.org/hub/nvidia_deeplearningexamples_resnet50/\n",
    "model = models.resnet50(pretrained=True)  \n",
    "\n",
    "# Move the model to GPU if available\n",
    "model = model.to(device)\n",
    "\n",
    "# sets the model to evaluation mode. \n",
    "# In this mode, the model behaves differently during inference compared to training. \n",
    "# For example, dropout layers are deactivated, batch normalization layers use the running statistics, \n",
    "#and the gradients are not computed for parameters. Setting the model to evaluation mode is important \n",
    "#to ensure consistent and accurate results during inference.\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a61cd8",
   "metadata": {},
   "source": [
    "# Cross entropy loss\n",
    "\n",
    "Cross entropy loss, also known as softmax loss or log loss, is a commonly used loss function in machine learning for multi-class classification tasks. It measures the dissimilarity between the predicted probability distribution and the true distribution of the target labels.\n",
    "\n",
    "In the context of classification, the cross entropy loss quantifies the difference between the predicted class probabilities and the actual class labels. It calculates the average negative log likelihood of the predicted probabilities for the true class labels. The goal is to minimize this loss function during the training process to improve the accuracy of the model.\n",
    "\n",
    "The formula for cross entropy loss involves taking the logarithm of the predicted probabilities, multiplying them with the true label's one-hot encoded representation, and summing the values across all classes. The loss is then averaged over the batch or the entire dataset.\n",
    "\n",
    "By optimizing the cross entropy loss, the model learns to assign higher probabilities to the correct class labels and lower probabilities to the incorrect ones. This loss function encourages the model to produce more confident predictions for the correct classes and penalizes uncertain or incorrect predictions.\n",
    "\n",
    "The cross entropy loss is widely used because it is effective for optimizing models in multi-class classification tasks. It provides a gradient signal that guides the model's parameters towards better predictions, facilitating the learning process. Many deep learning frameworks, including PyTorch, provide an implementation of the cross entropy loss for easy integration into the training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ca60b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function for multi-class classification tasks\n",
    "# initializing criterion with nn.CrossEntropyLoss(), we can use it later in the training loop \n",
    "# to compute and backpropagate the loss, updating the model's parameters to improve its performance \n",
    "# in the classification task.\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e8856451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The transforms.Compose() function is used to chain the transformations together into \n",
    "# a single pipeline, allowing for easy and efficient preprocessing of the input images before\n",
    "# feeding them into the model.\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),  # Resize the image to a square of size 256x256 pixels\n",
    "    transforms.CenterCrop(224),  # Crop the center of the image to a size of 224x224 pixels\n",
    "    transforms.ToTensor()  # Convert the image to a tensor representation\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "afe08ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the input image and convert it into a tensor\n",
    "# .unsqueeze(0): This adds an extra dimension to the tensor, making it a 4-dimensional tensor\n",
    "# with a batch size of 1. This is necessary as many deep learning models expect inputs in batch format.\n",
    "image = preprocess(Image.open('dog.jpg')).unsqueeze(0)  \n",
    "\n",
    "# Move the image and model to GPU if available\n",
    "image = image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8938ad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the extra batch dimension from the image tensor\n",
    "image2 = torch.squeeze(image, 0)\n",
    "\n",
    "# Create an instance of the ToPILImage transformation\n",
    "T = transforms.ToPILImage()\n",
    "\n",
    "# Convert the tensor image back to a PIL Image object\n",
    "img = T(image2)\n",
    "\n",
    "# Save the PIL Image as 'dog_original.jpg'\n",
    "img.save('dog_original.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b54ad128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the target label to 3\n",
    "# In the ResNet-50 architecture of torchvision.models, the label 3 corresponds to the specific class \"cat\" \n",
    "# within the ImageNet dataset. \n",
    "# https://deeplearning.cms.waikato.ac.nz/user-guide/class-maps/IMAGENET/\n",
    "target_label = 3 # tiger shark, Galeocerdo cuvieri\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb9d815",
   "metadata": {},
   "source": [
    "# epsilon (ε)\n",
    "\n",
    "In the context of adversarial attacks or perturbation-based techniques, epsilon (ε) is a small positive constant used to control the magnitude of perturbation applied to an image. It determines the maximum allowable change in pixel values for generating adversarial examples while maintaining visual similarity to the original image.\n",
    "\n",
    "The purpose of using epsilon is to strike a balance between the effectiveness of the attack and the perceptibility of the perturbation. By setting an appropriate value for epsilon, one can control the level of distortion introduced to the image to deceive a machine learning model.\n",
    "\n",
    "During adversarial attacks, the original image is perturbed by adding imperceptible perturbations to the pixel values. The magnitude of these perturbations is constrained by epsilon. A smaller epsilon value limits the amount of perturbation, making it less likely to be detected by humans but potentially less effective in fooling the model. On the other hand, a larger epsilon value allows for more significant perturbations, increasing the likelihood of misleading the model but potentially introducing noticeable visual changes.\n",
    "\n",
    "The choice of epsilon depends on factors such as the sensitivity of the targeted model, the specific attack technique being employed, and the desired trade-off between stealthiness and attack success rate. It is often determined through experimentation and fine-tuning to achieve the desired level of adversarial perturbation while minimizing perceptibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fad5eefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the value of epsilon to 0.03\n",
    "epsilon = 0.03\n",
    "\n",
    "# Enable gradient calculation for the image tensor\n",
    "image.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b7585d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform forward pass through the model\n",
    "output = model(image)\n",
    "\n",
    "# Calculate the loss using the criterion of the model's output compared to a target label\n",
    "loss = criterion(output, torch.tensor([target_label]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "95c97faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset gradients to zero\n",
    "model.zero_grad()\n",
    "\n",
    "# Perform backward pass to calculate gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "da9f2349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sign of the gradients of the image\n",
    "# Determine the direction of perturbation that maximizes the loss function.\n",
    "# image.grad represents the gradients of the image with respect to some loss function. Gradients capture \n",
    "# the direction and magnitude of the steepest ascent in the loss landscape.\n",
    "sign_grad = torch.sign(image.grad.data)\n",
    "\n",
    "# In the context of adversarial attacks, the sign of gradients helps determine the direction to perturb \n",
    "# the image in order to create an adversarial example that can mislead the model's predictions.\n",
    "\n",
    "# Generate the adversarial image by adding perturbation based on the sign of gradients\n",
    "adversarial_image = image + epsilon * sign_grad\n",
    "\n",
    "# By multiplying the perturbation (scaled by epsilon) with the sign of the gradients and adding it \n",
    "# to the original image, we introduce a small distortion to the image in the direction indicated \n",
    "# by the sign of the gradients. This distortion is intended to create an adversarial example that \n",
    "# can potentially fool a machine learning model.\n",
    "\n",
    "# The resulting adversarial_image is a tensor with the same shape as the original image, where \n",
    "# each pixel has been modified according to the sign of the gradients. The purpose is to create \n",
    "# a visually similar image that can lead to different predictions or misclassify the input when fed into \n",
    "# a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bd1608b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squeeze the dimensions of adversarial_image\n",
    "adversarial_image2 = torch.squeeze(adversarial_image, 0)\n",
    "\n",
    "# Convert the tensor to a PIL image\n",
    "T = transforms.ToPILImage()\n",
    "img = T(adversarial_image2)\n",
    "\n",
    "# Save the adversarial image to disk\n",
    "img.save('dog_adversarial.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5db91d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original prediction: 207\n",
      "Adversarial prediction: 222\n"
     ]
    }
   ],
   "source": [
    "# Predefined list of ImageNet labels\n",
    "# https://deeplearning.cms.waikato.ac.nz/user-guide/class-maps/IMAGENET/\n",
    "\n",
    "# Print the original prediction from the model\n",
    "# 207 - golden retriever\n",
    "print(\"Original prediction:\", torch.argmax(output).item())\n",
    "\n",
    "# Print the adversarial prediction obtained after applying the perturbation\n",
    "# 222 - kuvasz\n",
    "print(\"Adversarial prediction:\", adversarial_prediction.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761c5817",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
