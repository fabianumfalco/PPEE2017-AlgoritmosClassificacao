{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfd2cb92",
   "metadata": {},
   "source": [
    "# Adversarial Example\n",
    "\n",
    "An adversarial example refers to a specially crafted input data point that is intentionally designed to deceive a machine learning model. It is created by making subtle, often imperceptible modifications to the original input data, such as images, audio, or text, with the intention of causing the model to misclassify or produce incorrect outputs.\n",
    "\n",
    "The goal of an adversarial example is to exploit vulnerabilities or weaknesses in the model's decision-making process. By manipulating the input data in a strategic manner, an attacker can cause the model to make mistakes or produce undesired outcomes. These adversarial examples are typically created with the intent to deceive the model, rather than being naturally occurring data points.\n",
    "\n",
    "The concept of adversarial examples highlights the limitations and vulnerabilities of machine learning models. It demonstrates that even advanced models can be susceptible to manipulation and can be easily fooled by carefully crafted inputs. Adversarial examples raise concerns about the robustness, reliability, and security of machine learning systems in real-world applications.\n",
    "\n",
    "Understanding and studying adversarial examples is crucial for developing more robust and resilient machine learning models. Researchers and practitioners aim to develop defense mechanisms and techniques to detect and mitigate the impact of adversarial examples, improving the overall security and trustworthiness of machine learning systems.\n",
    "\n",
    "In summary, an adversarial example is a specially crafted input designed to exploit the vulnerabilities of a machine learning model, causing it to produce incorrect outputs or make mistakes. It serves as a test and a means to understand the limitations of machine learning systems and develop defenses against potential attacks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03858db9",
   "metadata": {},
   "source": [
    "# White-box Adversarial Examples\n",
    "\n",
    "White-box adversarial examples refer to adversarial attacks where the attacker has complete knowledge about the targeted machine learning model. In a white-box setting, the attacker has access to the model's architecture, parameters, and training data, enabling them to analyze and exploit its vulnerabilities effectively.\n",
    "\n",
    "To create a white-box adversarial example, the attacker can employ various optimization algorithms and techniques. They can use gradient information, obtained through backpropagation, to compute the direction and magnitude of perturbations that will maximize the model's prediction error. By iteratively adjusting the input data based on the gradient information, the attacker can craft an adversarial example that leads to a misclassification or an undesired output from the model.\n",
    "\n",
    "The main advantage of white-box attacks is the extensive knowledge available to the attacker. They can thoroughly analyze the model's weaknesses, understand its decision boundaries, and tailor the adversarial example accordingly. This access to detailed information allows for more precise and effective manipulation of the input data.\n",
    "\n",
    "White-box adversarial examples are particularly concerning because they simulate a scenario where the attacker has insider knowledge about the targeted model. This knowledge can be leveraged to design highly targeted attacks that exploit specific vulnerabilities, potentially causing severe consequences in real-world applications.\n",
    "\n",
    "Defending against white-box adversarial examples requires the development of robust and resilient machine learning models. Techniques like adversarial training, defensive distillation, and input preprocessing can enhance the model's ability to withstand such attacks. Additionally, ensuring model transparency, frequent security evaluations, and careful consideration of model architectures can help mitigate the impact of white-box attacks.\n",
    "\n",
    "In summary, white-box adversarial examples are crafted by attackers who possess complete knowledge of the targeted model. They exploit this knowledge to design precise and effective attacks, highlighting vulnerabilities in the model's decision-making process. Defending against white-box attacks requires robust model design, proactive security measures, and ongoing evaluation of the model's vulnerabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee1e33a",
   "metadata": {},
   "source": [
    "# Black-box Adversarial Examples \n",
    "Black-box adversarial examples refer to adversarial attacks where the attacker has limited or no knowledge about the targeted machine learning model. In a black-box setting, the attacker doesn't have access to the model's internal architecture, parameters, or training data. They can only interact with the model by providing inputs and observing its outputs.\n",
    "\n",
    "Creating black-box adversarial examples presents a greater challenge for the attacker compared to white-box attacks. Without detailed knowledge of the model, they need to rely on techniques that explore the model's behavior through input-output observations and feedback.\n",
    "\n",
    "One common approach in black-box attacks is the use of transferability. The attacker trains a surrogate model, either by collecting their own labeled data or using a publicly available dataset. This surrogate model mimics the behavior of the targeted model to some extent. By crafting adversarial examples on the surrogate model, the attacker assumes that these examples will also fool the targeted model due to the transferability of adversarial perturbations.\n",
    "\n",
    "Another technique employed in black-box attacks is the query-based method. The attacker can query the targeted model multiple times, providing carefully selected inputs and observing the corresponding outputs. By analyzing the model's responses and monitoring the changes in predictions, the attacker can gradually refine their understanding of the model's decision boundaries and craft adversarial examples.\n",
    "\n",
    "Black-box attacks are particularly challenging because they simulate scenarios where the attacker has limited information and has to work with restricted access to the targeted model. These attacks closely resemble real-world situations where models are deployed as services, and the internal workings are not exposed.\n",
    "\n",
    "Defending against black-box adversarial examples requires robustness even in the absence of specific knowledge about the attack. Techniques like adversarial training, ensemble methods, and input sanitization can enhance the model's resilience against unseen adversarial inputs. Additionally, monitoring and anomaly detection systems can help identify suspicious patterns and inputs that exhibit adversarial behavior.\n",
    "\n",
    "In summary, black-box adversarial examples are crafted by attackers who have limited or no knowledge about the targeted model. They rely on techniques like transferability and query-based methods to explore the model's behavior and create adversarial examples. Defending against black-box attacks requires robust models, proactive security measures, and techniques that can generalize to unseen adversarial inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1a4e6f",
   "metadata": {},
   "source": [
    "# Risks in Adversarial Example\n",
    "When it comes to applications of classification, such as text or image classification, adversarial examples pose several risks. Here are some of the major risks associated with adversarial examples:\n",
    "\n",
    "1. **Misclassification**: Adversarial examples can cause misclassification, where a model wrongly predicts the class or label of an input. By making small, imperceptible modifications to the input data, an attacker can trick the model into producing incorrect predictions. This misclassification can lead to severe consequences, especially in critical applications like autonomous vehicles or medical diagnosis. In autonomous vehicles, for example, misclassification can result in accidents or unsafe driving decisions. If an attacker can trick the object detection system into misclassifying a stop sign as a different object, it could lead to the vehicle failing to stop at an intersection, putting lives at risk. In healthcare applications, misclassification can impact patient diagnosis and treatment. If an adversarial example causes a medical image classifier to misclassify a tumor as non-cancerous or vice versa, it can have serious consequences for patient care, leading to delayed or incorrect treatments. Misclassification can also be exploited for malicious purposes. For instance, in spam email filtering, an attacker could craft adversarial examples to make spam emails appear as legitimate messages, bypassing the filtering system and potentially tricking users into revealing sensitive information or falling victim to phishing attacks.\n",
    "\n",
    "2. **Security Breaches**: Adversarial examples can be used as a means to exploit vulnerabilities in the classification system. Attackers can craft adversarial inputs specifically designed to bypass security measures, gain unauthorized access, or deceive the system for malicious purposes. This can compromise the integrity and security of sensitive data and systems. Here are some key aspects of the security risks posed by adversarial examples:\n",
    "\n",
    "    1. **Unauthorized access**: Adversarial examples can be crafted to deceive the system's authentication mechanisms, allowing attackers to gain unauthorized access to sensitive information or privileged functionalities. For example, in a face recognition system, an attacker may create an adversarial example that fools the system into granting access to an unauthorized individual.\n",
    "\n",
    "    2. **Data manipulation**: Adversarial examples can be used to manipulate the input data, leading to unauthorized modifications or tampering of data. This can have serious consequences in systems where data integrity is crucial, such as financial transactions or critical infrastructure monitoring. Attackers can exploit adversarial examples to manipulate data inputs and deceive the system into making incorrect decisions or actions.\n",
    "\n",
    "    3. **Evasion of security measures**: Adversarial examples can be employed to bypass security measures and evade detection or filtering mechanisms. For instance, in malware detection systems, attackers can create adversarial examples that obfuscate malicious code, allowing them to evade detection by antivirus software or intrusion detection systems.\n",
    "\n",
    "    4. **Malware injection**: Adversarial examples can be used to inject malicious content or code into the system. By crafting adversarial inputs that exploit vulnerabilities in the system's processing pipeline, attackers can inject malware, viruses, or other harmful payloads, which can compromise the system's integrity, steal sensitive information, or cause system malfunctions.\n",
    "\n",
    "    5. **Privacy breaches**: Adversarial examples can lead to privacy breaches by revealing sensitive information or extracting confidential data. For example, in natural language processing applications, attackers can create adversarial examples that subtly embed sensitive information within seemingly harmless text, bypassing privacy protection mechanisms and potentially extracting confidential data. By manipulating inputs, attackers might be able to extract sensitive features or fool the system into revealing confidential information. This poses a significant risk in applications that deal with personal data or sensitive information.\n",
    "\n",
    "3. **Model Robustness Evaluation**: Adversarial examples serve as a critical tool for evaluating the robustness and reliability of classification models. By exposing models to adversarial attacks, researchers and developers can identify vulnerabilities, weaknesses, and limitations in the models' performance. Failure to account for adversarial examples during model evaluation can lead to overestimation of performance and create a false sense of security. Here are some key points to consider regarding the risk of model robustness evaluation:\n",
    "\n",
    "    1. **Performance degradation**: Adversarial examples can cause significant performance degradation in classification models. Even a small perturbation in the input data can lead to misclassifications or incorrect predictions. This poses a risk in real-world applications where accurate and reliable predictions are crucial, such as autonomous vehicles or medical diagnosis systems.\n",
    "\n",
    "    2. **Evaluation bias**: Adversarial examples can introduce bias in the evaluation of machine learning models. When evaluating model performance using standard test sets, the absence of adversarial examples can lead to overestimating the model's accuracy and robustness. This creates a false sense of security and hinders the detection of vulnerabilities and weaknesses in the model.\n",
    "\n",
    "    3. **Lack of generalization**: Adversarial examples challenge the generalization capabilities of machine learning models. Models that perform well on clean, natural data may fail when exposed to adversarial examples. This risk is particularly concerning as it raises questions about the reliability and real-world applicability of classification models.\n",
    "\n",
    "    4. **Lack of standard evaluation metrics**: There is a need for standardized evaluation metrics to assess the robustness of classification models against adversarial attacks. Current evaluation methods often rely on ad-hoc metrics or specific datasets, which can limit the comparability and reproducibility of results. Developing robust evaluation frameworks and metrics that capture the model's resilience against adversarial examples is essential.\n",
    "\n",
    "4. **Transferability**: Adversarial examples can exhibit transferability, meaning that an adversarial example designed to fool one model can also fool other models, even if they have different architectures or were trained on different datasets. This transferability poses a risk across different systems and implementations, as an attack designed for one model can potentially affect multiple models or classifiers. Here are some key points to consider regarding the risk of transferability:\n",
    "\n",
    "    1. **Generalization of attacks**: Adversarial examples can exhibit a certain level of transferability, meaning the same perturbation can successfully fool multiple models. This is concerning because it indicates that adversarial attacks can generalize across different models, posing a broader threat to the security and reliability of classification systems.\n",
    "\n",
    "    2. **Attack potency**: Adversarial examples that demonstrate high transferability are particularly potent as they can compromise the performance of various machine learning models simultaneously. Adversaries can craft a single adversarial example that can deceive multiple models, potentially causing widespread disruptions and misclassifications.\n",
    "\n",
    "    3. **Practical implications**: The transferability of adversarial examples has practical implications in real-world scenarios. If an attacker can successfully generate an adversarial example to fool one model, they can potentially exploit the same vulnerability to deceive other models deployed in different systems or organizations. This poses a serious security concern, especially in applications where decisions based on machine learning predictions have significant consequences.\n",
    "\n",
    "    4. **Robustness evaluation challenges**: Transferability also raises challenges in evaluating the robustness of classification models. A model's performance against known adversarial examples may not reflect its vulnerability to unseen attacks with high transferability. Robustness evaluation must consider the potential for transferable attacks to ensure models are adequately protected.\n",
    "\n",
    "    5. **Adversarial example generation methods**: Transferability underscores the importance of studying and understanding the underlying mechanisms behind adversarial example generation. By uncovering the transferability properties of adversarial examples, researchers can develop more effective defense mechanisms and strategies to mitigate the risks associated with such attacks.\n",
    "\n",
    "5. **Social Engineering**: Adversarial examples can be used as a form of social engineering, manipulating the perception of systems to deceive users or gain their trust. By crafting adversarial inputs, attackers can create a false sense of credibility or authority, leading users to make decisions based on incorrect or manipulated information.\n",
    "\n",
    "6. **Ethical Implications**: The existence of adversarial examples raises ethical concerns related to the trustworthiness and fairness of classification systems. Adversarial attacks can disproportionately affect certain groups or bias decision-making processes. Addressing these ethical implications and ensuring fairness in classification systems is crucial to maintain public trust and avoid discriminatory outcomes.Here's a closer look at the potential risks and concerns:\n",
    "\n",
    "    1. **Bias amplification**: Adversarial examples have the potential to amplify existing biases present in classification models or introduce new biases. This can lead to discriminatory outcomes, perpetuate social inequalities, and result in unfair treatment of individuals or groups.\n",
    "\n",
    "    2. **Manipulation of decision-making**: Adversarial examples can be used to manipulate decision-making processes, leading to ethical concerns. For example, in sensitive domains like hiring or loan approvals, adversarial examples can unfairly influence the outcomes, resulting in biased or discriminatory decisions.\n",
    "\n",
    "    3. **Privacy violations**: Crafting adversarial examples requires access to sensitive data and knowledge about the targeted classification model. The use of such techniques raises concerns about privacy violations and unauthorized access to personal information, as attackers may exploit vulnerabilities to gain unauthorized access.\n",
    "\n",
    "    4. **Trust and transparency**: Adversarial examples challenge the trust and transparency of classification systems. If users become aware of the susceptibility of these systems to manipulation, they may lose trust in their outputs, which can have far-reaching implications in various domains, including healthcare, finance, and security.\n",
    "\n",
    "    5. **Accountability and responsibility**: Adversarial examples raise questions about accountability and responsibility. When these examples lead to harmful outcomes, it becomes challenging to determine who should be held accountable—whether it is the creator of the adversarial example, the classification model developer, or the system operator.\n",
    "\n",
    "    6. **Malicious use**: Adversarial examples can be used with malicious intent, such as spreading misinformation, launching targeted attacks, or compromising the integrity of classification systems. This poses ethical concerns as these techniques can be weaponized for personal gain or to cause harm.\n",
    "\n",
    "    7. **Implications for society**: The widespread use of classification systems in critical areas such as healthcare, criminal justice, and social welfare has significant societal implications. If adversarial examples are not adequately addressed, they can undermine public trust, exacerbate social divisions, and compromise the fairness and integrity of societal processes.\n",
    "\n",
    "7. **Adversarial Attacks on Training Data**: Adversarial examples can also be used to manipulate the training data itself. By introducing subtle perturbations or modifications to the training samples, attackers can influence the learning process and bias the model's behavior. This can lead to biased predictions, compromised model performance, or unintended consequences during deployment. Here's a closer look at this risk:\n",
    "\n",
    "    1. **Data integrity compromise**: Adversarial attacks on training data involve injecting maliciously crafted examples into the dataset used to train classification models. These examples are carefully designed to manipulate the learning process and potentially compromise the integrity of the trained model. By subtly modifying a small number of training examples, an attacker can influence the model's decision boundaries and introduce vulnerabilities.\n",
    "\n",
    "    2. **Model vulnerability exploitation**: Adversarial attacks on training data aim to exploit vulnerabilities in the learning algorithm or model architecture. By carefully crafting perturbations or alterations to the training data, an attacker can deceive the model during the learning process, leading to compromised performance or targeted misclassification.\n",
    "\n",
    "    3. **Generalization gap**: Adversarial attacks on training data can widen the gap between a model's performance on clean, real-world data and its performance on adversarial examples. This phenomenon, known as the \"generalization gap,\" indicates that models trained on clean data may fail to generalize robustly to adversarial inputs, even if they exhibit high accuracy on normal test data.\n",
    "\n",
    "    4. **Adversarial overfitting**: When training data is contaminated with adversarial examples, the model may inadvertently learn to overfit to these specific instances rather than capturing the underlying patterns in the data. This can result in poor generalization to real-world scenarios and reduced overall performance.\n",
    "\n",
    "    5. **Data poisoning attacks**: Adversarial attacks on training data can be used as a form of data poisoning, where attackers aim to manipulate the model's learning process by injecting malicious examples. These poisoned examples can bias the model's decision boundaries or lead to targeted misclassifications, compromising the model's integrity and reliability.\n",
    "\n",
    "    6. **Transferability to other models**: Adversarial examples created during training can also transfer to other models that are trained on similar or related data. This means that an attacker's carefully crafted adversarial examples can potentially fool multiple models, increasing the impact and reach of the attack.\n",
    "\n",
    "8. **Legal and Regulatory Compliance**: The presence of adversarial examples can have legal and regulatory implications, particularly in domains where accuracy and reliability are critical. Compliance with regulations, such as data protection or safety standards, may require thorough evaluation and mitigation strategies against adversarial attacks to ensure the integrity and trustworthiness of the classification system. Here's an overview of the associated risks:\n",
    "\n",
    "    1. **Privacy and data protection**: Adversarial attacks may involve the manipulation or unauthorized access to sensitive data used for training or inference. If the attack results in the exposure or compromise of personal or confidential information, it can lead to legal and regulatory violations related to privacy and data protection laws. Organizations must ensure that appropriate measures are in place to safeguard user data and comply with relevant regulations.\n",
    "\n",
    "    2. **Liability for misclassification**: Adversarial attacks can result in misclassification of data, leading to potential legal consequences. For example, in applications such as medical diagnosis or autonomous vehicles, misclassification due to adversarial examples can result in incorrect decisions or harm to individuals. Organizations may be held liable for any damages or injuries caused by such misclassifications, which could lead to legal claims and financial repercussions.\n",
    "\n",
    "    3. **Intellectual property infringement**: Adversarial attacks can involve the unauthorized modification or manipulation of copyrighted or proprietary data used for training models. If an attacker gains access to protected intellectual property and uses it to craft adversarial examples, it can result in intellectual property infringement claims. Organizations must ensure that they have proper safeguards in place to protect their intellectual property and comply with copyright laws.\n",
    "\n",
    "    4. **Compliance with fairness and discrimination regulations**: Adversarial attacks can have implications for fairness and discrimination in classification systems. If the attack disproportionately affects certain protected groups or leads to biased outcomes, it can violate anti-discrimination laws or regulations. Organizations must be mindful of the potential impact of adversarial attacks on fairness and take steps to address any biases or discriminatory effects.\n",
    "\n",
    "    5. **Compliance with industry-specific regulations**: Different industries may have specific regulations and standards that govern the use of classification systems. Adversarial attacks can impact compliance with these regulations, such as in the healthcare sector, finance industry, or autonomous driving. Organizations must ensure that their classification systems meet the required standards and comply with industry-specific regulations to avoid legal and regulatory penalties.\n",
    "\n",
    "It is important to note that the risks associated with adversarial examples depend on the specific application, the potential impact of misclassification or manipulation, and the threat landscape. Understanding and mitigating these risks is essential for the development and deployment of robust and secure classification systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257d7920",
   "metadata": {},
   "source": [
    "# How to Protect Classification Systems against Adversarial Attacks\n",
    "To protect classification systems against adversarial attacks, various measures can be implemented. Here are several protections that can help mitigate the risks:\n",
    "\n",
    "1. **Adversarial training**: One approach is to incorporate adversarial examples into the training process. By generating and including adversarial examples during training, the model learns to be robust against such attacks. Adversarial training introduces perturbations to the training data, allowing the model to learn to recognize and classify both clean and adversarial inputs.However, it is important to note that it comes with certain trade-offs. Adversarial training can increase computational complexity, as generating adversarial examples and augmenting the dataset require additional resources. Additionally, the defense may not generalize perfectly to all types of adversarial attacks, and there is an ongoing arms race between attackers and defenders.To effectively implement adversarial training, it is recommended to carefully select appropriate attack methods, set appropriate hyperparameters, and regularly update the defense strategy to adapt to new attack techniques.\n",
    "\n",
    "2. **Robust model architectures**: Designing robust model architectures can make it more difficult for adversarial examples to manipulate the system. Architectures like adversarially trained models or defensive distillation can improve the model's resistance to attacks by incorporating additional layers or regularization techniques.Here are some key aspects and techniques related to robust model architectures:\n",
    "\n",
    "   1. **Deep architectures**: Deep neural networks with multiple layers have shown improved resilience against adversarial attacks compared to shallow models. The depth of the network allows for more complex representations and feature extraction, making it harder for adversarial perturbations to manipulate the input data.\n",
    "\n",
    "   2. **Regularization techniques**: Regularization techniques, such as Dropout and Batch Normalization, can be used to improve model robustness. Dropout randomly sets a fraction of input units to zero during training, which acts as a form of regularization and reduces the network's reliance on specific features. Batch Normalization helps to stabilize and normalize the input to each layer, making the network more resilient to input variations.\n",
    "\n",
    "   3. **Adaptive activation functions**: Using activation functions that are less sensitive to adversarial perturbations can enhance the robustness of the model. For example, using activation functions like the Scaled Exponential Linear Unit (SELU) or the Swish activation function can help mitigate the impact of adversarial perturbations.\n",
    "\n",
    "   4. **Adaptive gradient regularization**: Adaptive gradient regularization methods, such as Adversarial Logit Pairing (ALP) and Virtual Adversarial Training (VAT), introduce additional regularization terms in the loss function to encourage model predictions to be stable around the local region. These techniques make the model less sensitive to small adversarial perturbations by penalizing changes in the model's predictions.\n",
    "\n",
    "3. **Defensive distillation**: Defensive distillation is a technique that involves training a model on soft labels rather than hard labels. Soft labels are probabilities assigned to each class instead of binary values. This can make the model more resilient to adversarial attacks as the soft labels provide a smoother decision space.\n",
    "\n",
    "4. **Input preprocessing**: Applying input preprocessing techniques can enhance the model's resilience. Techniques like input normalization or randomization can reduce the effectiveness of adversarial perturbations. Preprocessing methods such as spatial transformation or noise injection can also make the model more robust to adversarial examples.\n",
    "\n",
    "5. **Adversarial example detection**: Implementing mechanisms to detect adversarial examples during inference can help identify and reject malicious inputs. Techniques such as defensive distillation, outlier detection, or similarity-based methods can be used to flag potential adversarial inputs and trigger appropriate actions.\n",
    "\n",
    "6. **Ensemble models**: Employing ensemble models, which combine predictions from multiple models, can increase robustness. Adversarial examples are more likely to cause disagreements among different models, making it harder for an attacker to manipulate the system consistently.\n",
    "\n",
    "7. **Adversarial example-aware defenses**: Researchers have developed specific defense mechanisms tailored to counter adversarial attacks. These defenses include methods like defensive distillation, randomized smoothing, or certified defenses, which provide provable guarantees against adversarial examples.\n",
    "\n",
    "8. **Monitoring and updating**: Continuously monitoring the performance of classification models and updating them with new data can help identify and mitigate potential vulnerabilities. Regular model evaluation and retraining can improve the system's ability to handle adversarial examples.\n",
    "\n",
    "9. **Data augmentation**: Increasing the diversity and size of the training dataset through data augmentation techniques can enhance the model's generalization capability and make it more resilient to adversarial attacks.\n",
    "\n",
    "10. **Adversarial example-aware training data collection**: When collecting training data, it is important to consider the potential presence of adversarial examples. Collecting diverse and realistic data that includes potential adversarial inputs can improve the model's robustness.\n",
    "\n",
    "11. **Regular security audits**: Conducting regular security audits and assessments can help identify potential vulnerabilities and evaluate the effectiveness of existing defenses against adversarial attacks. This can involve internal or external security evaluations, penetration testing, or red teaming exercises.\n",
    "\n",
    "12. **User education and awareness**: Educating users about the existence and potential impact of adversarial examples can help raise awareness and reduce the likelihood of successful attacks. Users can be encouraged to report suspicious or potentially adversarial inputs, fostering a collaborative approach to system security.\n",
    "\n",
    "It is important to note that while these protections can enhance the system's resilience to adversarial attacks, there is no foolproof defense. Adversarial attacks continue to evolve, and researchers are constantly exploring new techniques. Therefore, it is crucial to stay updated on the latest advancements in adversarial defense and continuously improve system security to mitigate the risks associated with adversarial examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea2375a",
   "metadata": {},
   "source": [
    "# Notebook \n",
    "The objective of this notebook is to perform a series of operations using a pre-trained model (ResNet-50) to generate an adversarial image from an input image. Let's go through the steps of the code:\n",
    "\n",
    "1. Import the necessary libraries:\n",
    "2. Load a pre-trained model:\n",
    "3. Set up the model for evaluation:\n",
    "4. Define the loss function:\n",
    "5. Define preprocessing transformations:\n",
    "6. Load and preprocess the image:\n",
    "7. Save the original image:\n",
    "8. Define the target label:\n",
    "9. Set up the image tensor for gradient calculation:\n",
    "10. Make a prediction on the original image:\n",
    "11. Calculate the loss and backpropagate the gradient:\n",
    "12. Calculate the sign gradients:\n",
    "13. Generate the adversarial image:\n",
    "14. Save the adversarial image:\n",
    "15. Make a prediction on the adversarial image:\n",
    "16. Print the original prediction:\n",
    "\n",
    "The code is typically used to demonstrate the process of creating an adversarial image using a pre-trained model and PyTorch. It highlights the steps involved in generating the adversarial image and evaluating its impact on the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "264e5add",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the watermark package.\n",
    "# This package is used to record the versions of other packages used in this Jupyter notebook.\n",
    "# https://github.com/rasbt/watermark\n",
    "!pip install -q -U watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3bc9f617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # Importing the PyTorch library for deep learning\n",
    "import torch.nn as nn  # Importing the neural network module of PyTorch\n",
    "import torch.optim as optim  # Importing the optimization module of PyTorch\n",
    "import torchvision.transforms as transforms  # Importing image transformations from torchvision\n",
    "import torchvision.models as models  # Importing pre-trained models from torchvision\n",
    "from PIL import Image  # Importing the Python Imaging Library for image processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "174bd5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings  # Import the warnings library for warning control and management\n",
    "\n",
    "# Disable all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "afc9ce2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Fabiano Falcão\n",
      "\n",
      "Website: https://fabianumfalco.github.io/\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.10.6\n",
      "IPython version      : 8.11.0\n",
      "\n",
      "PIL        : 9.0.1\n",
      "torch      : 2.0.0\n",
      "torchvision: 0.15.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the watermark extension to display information about the Python version and installed packages.\n",
    "%reload_ext watermark\n",
    "\n",
    "# Display the versions of Python and installed packages.\n",
    "%watermark -a 'Fabiano Falcão' -ws \"https://fabianumfalco.github.io/\" --python --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "769e8b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set the device to CUDA (GPU) if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  \n",
    "print(f\"Using device: {device}\")  # Print the device being used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6caac56",
   "metadata": {},
   "source": [
    "# ResNet-50\n",
    "\n",
    "The ResNet-50 is a convolutional neural network (CNN) architecture that was proposed as a solution to the problem of vanishing gradients in deep network training. It was first introduced in the paper \"Deep Residual Learning for Image Recognition\" by Kaiming He et al. in 2015.\n",
    "\n",
    "The ResNet-50 consists of 50 layers, including convolutional, activation, and pooling layers. The main innovation of ResNet-50 is the use of residual connections, which allow the gradient information to be directly propagated through the layers, even in deep networks. These residual connections enable the model to learn richer and deeper representations of the images, thereby improving the accuracy of object recognition.\n",
    "\n",
    "ResNet-50 was trained on a large dataset, such as ImageNet, which contains millions of images across various classes. This allowed the model to learn general features of a wide range of objects and textures. As a result, ResNet-50 has become a benchmark model for image classification tasks, achieving state-of-the-art performance on object recognition benchmarks.\n",
    "\n",
    "Furthermore, due to its deep learning capacity and rich representations, ResNet-50 has also been used as a base for transfer learning, where the learned features can be transferred to related tasks such as object detection and semantic segmentation.\n",
    "\n",
    "In summary, ResNet-50 is a powerful CNN architecture that overcame the limitations of deep networks by introducing residual connections. It excelled in image classification tasks and has served as a foundation for many other computer vision applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d94d15e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pre-trained ResNet-50 model from torchvision\n",
    "# https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html\n",
    "# https://pytorch.org/hub/nvidia_deeplearningexamples_resnet50/\n",
    "model = models.resnet50(pretrained=True)  \n",
    "\n",
    "# Move the model to GPU if available\n",
    "model = model.to(device)\n",
    "\n",
    "# sets the model to evaluation mode. \n",
    "# In this mode, the model behaves differently during inference compared to training. \n",
    "# For example, dropout layers are deactivated, batch normalization layers use the running statistics, \n",
    "#and the gradients are not computed for parameters. Setting the model to evaluation mode is important \n",
    "#to ensure consistent and accurate results during inference.\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46495971",
   "metadata": {},
   "source": [
    "# Cross entropy loss\n",
    "\n",
    "Cross entropy loss, also known as softmax loss or log loss, is a commonly used loss function in machine learning for multi-class classification tasks. It measures the dissimilarity between the predicted probability distribution and the true distribution of the target labels.\n",
    "\n",
    "In the context of classification, the cross entropy loss quantifies the difference between the predicted class probabilities and the actual class labels. It calculates the average negative log likelihood of the predicted probabilities for the true class labels. The goal is to minimize this loss function during the training process to improve the accuracy of the model.\n",
    "\n",
    "The formula for cross entropy loss involves taking the logarithm of the predicted probabilities, multiplying them with the true label's one-hot encoded representation, and summing the values across all classes. The loss is then averaged over the batch or the entire dataset.\n",
    "\n",
    "By optimizing the cross entropy loss, the model learns to assign higher probabilities to the correct class labels and lower probabilities to the incorrect ones. This loss function encourages the model to produce more confident predictions for the correct classes and penalizes uncertain or incorrect predictions.\n",
    "\n",
    "The cross entropy loss is widely used because it is effective for optimizing models in multi-class classification tasks. It provides a gradient signal that guides the model's parameters towards better predictions, facilitating the learning process. Many deep learning frameworks, including PyTorch, provide an implementation of the cross entropy loss for easy integration into the training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ca60b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function for multi-class classification tasks\n",
    "# initializing criterion with nn.CrossEntropyLoss(), we can use it later in the training loop \n",
    "# to compute and backpropagate the loss, updating the model's parameters to improve its performance \n",
    "# in the classification task.\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e8856451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The transforms.Compose() function is used to chain the transformations together into \n",
    "# a single pipeline, allowing for easy and efficient preprocessing of the input images before\n",
    "# feeding them into the model.\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),  # Resize the image to a square of size 256x256 pixels\n",
    "    transforms.CenterCrop(224),  # Crop the center of the image to a size of 224x224 pixels\n",
    "    transforms.ToTensor()  # Convert the image to a tensor representation\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "afe08ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the input image and convert it into a tensor\n",
    "# .unsqueeze(0): This adds an extra dimension to the tensor, making it a 4-dimensional tensor\n",
    "# with a batch size of 1. This is necessary as many deep learning models expect inputs in batch format.\n",
    "image = preprocess(Image.open('dog.jpg')).unsqueeze(0)  \n",
    "\n",
    "# Move the image and model to GPU if available\n",
    "image = image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8938ad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the extra batch dimension from the image tensor\n",
    "image2 = torch.squeeze(image, 0)\n",
    "\n",
    "# Create an instance of the ToPILImage transformation\n",
    "T = transforms.ToPILImage()\n",
    "\n",
    "# Convert the tensor image back to a PIL Image object\n",
    "img = T(image2)\n",
    "\n",
    "# Save the PIL Image as 'dog_original.jpg'\n",
    "img.save('dog_original.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b54ad128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the target label to 3\n",
    "# In the ResNet-50 architecture of torchvision.models, the label 3 corresponds to the specific class \"cat\" \n",
    "# within the ImageNet dataset. \n",
    "# https://deeplearning.cms.waikato.ac.nz/user-guide/class-maps/IMAGENET/\n",
    "target_label = 3 # tiger shark, Galeocerdo cuvieri\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f48e91",
   "metadata": {},
   "source": [
    "# epsilon (ε)\n",
    "\n",
    "In the context of adversarial attacks or perturbation-based techniques, epsilon (ε) is a small positive constant used to control the magnitude of perturbation applied to an image. It determines the maximum allowable change in pixel values for generating adversarial examples while maintaining visual similarity to the original image.\n",
    "\n",
    "The purpose of using epsilon is to strike a balance between the effectiveness of the attack and the perceptibility of the perturbation. By setting an appropriate value for epsilon, one can control the level of distortion introduced to the image to deceive a machine learning model.\n",
    "\n",
    "During adversarial attacks, the original image is perturbed by adding imperceptible perturbations to the pixel values. The magnitude of these perturbations is constrained by epsilon. A smaller epsilon value limits the amount of perturbation, making it less likely to be detected by humans but potentially less effective in fooling the model. On the other hand, a larger epsilon value allows for more significant perturbations, increasing the likelihood of misleading the model but potentially introducing noticeable visual changes.\n",
    "\n",
    "The choice of epsilon depends on factors such as the sensitivity of the targeted model, the specific attack technique being employed, and the desired trade-off between stealthiness and attack success rate. It is often determined through experimentation and fine-tuning to achieve the desired level of adversarial perturbation while minimizing perceptibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fad5eefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the value of epsilon to 0.03\n",
    "epsilon = 0.03\n",
    "\n",
    "# Enable gradient calculation for the image tensor\n",
    "image.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b7585d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform forward pass through the model\n",
    "output = model(image)\n",
    "\n",
    "# Calculate the loss using the criterion of the model's output compared to a target label\n",
    "loss = criterion(output, torch.tensor([target_label]).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "95c97faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset gradients to zero\n",
    "model.zero_grad()\n",
    "\n",
    "# Perform backward pass to calculate gradients\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "da9f2349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sign of the gradients of the image\n",
    "# Determine the direction of perturbation that maximizes the loss function.\n",
    "# image.grad represents the gradients of the image with respect to some loss function. Gradients capture \n",
    "# the direction and magnitude of the steepest ascent in the loss landscape.\n",
    "sign_grad = torch.sign(image.grad.data)\n",
    "\n",
    "# In the context of adversarial attacks, the sign of gradients helps determine the direction to perturb \n",
    "# the image in order to create an adversarial example that can mislead the model's predictions.\n",
    "\n",
    "# Generate the adversarial image by adding perturbation based on the sign of gradients\n",
    "adversarial_image = image + epsilon * sign_grad\n",
    "\n",
    "# By multiplying the perturbation (scaled by epsilon) with the sign of the gradients and adding it \n",
    "# to the original image, we introduce a small distortion to the image in the direction indicated \n",
    "# by the sign of the gradients. This distortion is intended to create an adversarial example that \n",
    "# can potentially fool a machine learning model.\n",
    "\n",
    "# The resulting adversarial_image is a tensor with the same shape as the original image, where \n",
    "# each pixel has been modified according to the sign of the gradients. The purpose is to create \n",
    "# a visually similar image that can lead to different predictions or misclassify the input when fed into \n",
    "# a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bd1608b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squeeze the dimensions of adversarial_image\n",
    "adversarial_image2 = torch.squeeze(adversarial_image, 0)\n",
    "\n",
    "# Convert the tensor to a PIL image\n",
    "T = transforms.ToPILImage()\n",
    "img = T(adversarial_image2)\n",
    "\n",
    "# Save the adversarial image to disk\n",
    "img.save('dog_adversarial.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5db91d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original prediction: 207\n",
      "Adversarial prediction: 222\n"
     ]
    }
   ],
   "source": [
    "# Predefined list of ImageNet labels\n",
    "# https://deeplearning.cms.waikato.ac.nz/user-guide/class-maps/IMAGENET/\n",
    "\n",
    "# Print the original prediction from the model\n",
    "# 207 - golden retriever\n",
    "print(\"Original prediction:\", torch.argmax(output).item())\n",
    "\n",
    "# Print the adversarial prediction obtained after applying the perturbation\n",
    "# 222 - kuvasz\n",
    "print(\"Adversarial prediction:\", adversarial_prediction.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
