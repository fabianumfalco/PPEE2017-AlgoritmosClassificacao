{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the watermark package.\n",
    "# This package is used to record the versions of other packages used in this Jupyter notebook.\n",
    "# https://github.com/rasbt/watermark\n",
    "!pip install -q -U watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "N6oyS9q9FGaA"
   },
   "outputs": [],
   "source": [
    "import os                         # Used for operating system related functionalities\n",
    "import time  # Provides time-related functions for tracking training and testing time\n",
    "import torch                      # The main PyTorch library\n",
    "import torch.nn as nn             # Provides various neural network layers and functions\n",
    "import torch.optim as optim       # Contains different optimization algorithms\n",
    "import torchvision.datasets as datasets  # Provides access to popular datasets\n",
    "import torchvision.transforms as transforms  # Offers various image transformations\n",
    "from torch.utils.data import DataLoader    # Helps with efficient data loading\n",
    "from torchvision.utils import save_image   # Used for saving generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Fabiano Falcão\n",
      "\n",
      "Website: https://fabianumfalco.github.io/\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.10.6\n",
      "IPython version      : 8.11.0\n",
      "\n",
      "torch      : 2.0.0\n",
      "torchvision: 0.15.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the watermark extension to display information about the Python version and installed packages.\n",
    "%reload_ext watermark\n",
    "\n",
    "# Display the versions of Python and installed packages.\n",
    "%watermark -a 'Fabiano Falcão' -ws \"https://fabianumfalco.github.io/\" --python --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ewg0AyyNuYpY"
   },
   "outputs": [],
   "source": [
    "# Define the Discriminator\n",
    "##############################################################################################################\n",
    "# The code defines a discriminator module using a sequential model in PyTorch. \n",
    "# The discriminator takes an image as input and outputs a single value indicating \n",
    "# the probability of the input being real or fake. The model consists of three linear layers \n",
    "# with leaky ReLU activation functions and a final sigmoid activation function. \n",
    "# The forward method applies the model to the input and returns the output.\n",
    "##############################################################################################################\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, image_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # Define the model architecture\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(image_size, 512),     # Linear layer: input size is image_size, output size is 512\n",
    "            nn.LeakyReLU(0.2),               # LeakyReLU activation function with a negative slope of 0.2\n",
    "            nn.Linear(512, 256),             # Linear layer: input size is 512, output size is 256\n",
    "            nn.LeakyReLU(0.2),               # LeakyReLU activation function with a negative slope of 0.2\n",
    "            nn.Linear(256, 1),               # Linear layer: input size is 256, output size is 1\n",
    "            nn.Sigmoid()                     # Sigmoid activation function to squash the output to the range [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)   # Pass the input through the model\n",
    "        return x            # Return the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "AAXowY3mucuC"
   },
   "outputs": [],
   "source": [
    "# Define the Generator\n",
    "##############################################################################################################\n",
    "# The code defines a generator module using a sequential model in PyTorch. \n",
    "# The generator takes a latent vector as input and generates an image as output. \n",
    "# The model consists of three linear layers with leaky ReLU activation functions and \n",
    "# a final Tanh activation function. The forward method applies the model to the input and \n",
    "# returns the output. The Tanh activation function is used to ensure that \n",
    "# the generated image has pixel values in the range [-1, 1].\n",
    "##############################################################################################################\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_size, image_size):\n",
    "        super(Generator, self).__init__()\n",
    "        # Define the model architecture\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_size, 256),      # Linear layer: input size is latent_size, output size is 256\n",
    "            nn.LeakyReLU(0.2),                 # LeakyReLU activation function with a negative slope of 0.2\n",
    "            nn.Linear(256, 512),               # Linear layer: input size is 256, output size is 512\n",
    "            nn.LeakyReLU(0.2),                 # LeakyReLU activation function with a negative slope of 0.2\n",
    "            nn.Linear(512, image_size),        # Linear layer: input size is 512, output size is image_size\n",
    "            nn.Tanh()                          # Tanh activation function to squash the output to the range [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)   # Pass the input through the model\n",
    "        return x            # Return the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "eBge9R51ufeP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hyperparameters\n",
    "# latent_size: The size of the latent vector used as input for the generator. \n",
    "#              It determines the dimensionality of the random noise used to generate images.\n",
    "# image_size: The size of the input image. In this case, it's set to 28 * 28, corresponding to \n",
    "#             the size of the images in the MNIST dataset.\n",
    "# batch_size: The number of samples processed in each mini-batch during training. \n",
    "#             It affects the speed and memory requirements of the training process.\n",
    "# epochs: The number of times the entire dataset will be iterated over during training. \n",
    "#         One epoch represents a complete pass through the dataset.\n",
    "\n",
    "latent_size = 100    # Size of the latent vector for the generator input\n",
    "image_size = 28 * 28  # Size of the input image (28x28 for MNIST)\n",
    "batch_size = 128     # Number of samples in each mini-batch\n",
    "epochs = 100         # Number of training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R3mHGdalujQI",
    "outputId": "a5194e42-b566-436f-e473-1b200c804b21"
   },
   "outputs": [],
   "source": [
    "# Load the MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                                     # Convert images to tensors\n",
    "    transforms.Normalize((0.5,), (0.5,))                       # Normalize the pixel values to the range [-1, 1]\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data/mnist_data/',             # Root directory to store the dataset\n",
    "                              train=True,                      # Load the training set\n",
    "                              transform=transform,             # Apply the specified transformations\n",
    "                              download=True)                   # Download the dataset if it's not already downloaded\n",
    "\n",
    "train_loader = DataLoader(train_dataset,                        # Wrap the training dataset with a data loader\n",
    "                          batch_size=batch_size,                # Number of samples in each mini-batch\n",
    "                          shuffle=True)                         # Shuffle the data at the beginning of each epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1oULyRoHuoPN"
   },
   "outputs": [],
   "source": [
    "# Initialize the discriminator and generator\n",
    "\n",
    "# Create an instance of the Discriminator model and move it to the device (GPU or CPU)\n",
    "discriminator = Discriminator(image_size).to(device)\n",
    "\n",
    "# Create an instance of the Generator model and move it to the device (GPU or CPU)\n",
    "generator = Generator(latent_size, image_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0yiIp-R2usFy"
   },
   "outputs": [],
   "source": [
    "# Loss function and optimizers\n",
    "criterion = nn.BCELoss()   # Binary Cross Entropy Loss function for training the discriminator and generator\n",
    "\n",
    "# Adam optimizer for updating the discriminator's parameters\n",
    "discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002)   \n",
    "# Adam optimizer for updating the generator's parameters\n",
    "generator_optimizer = optim.Adam(generator.parameters(), lr=0.0002)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HJKi9IpcuwUv",
    "outputId": "57ac8c08-fc42-482a-feb1-b4d8671603ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/100], Batch [0/469], Discriminator Loss: 1.4366, Generator Loss: 0.6688\n",
      "Epoch [0/100], Batch [100/469], Discriminator Loss: 0.5819, Generator Loss: 1.6277\n",
      "Epoch [0/100], Batch [200/469], Discriminator Loss: 0.6051, Generator Loss: 1.9980\n",
      "Epoch [0/100], Batch [300/469], Discriminator Loss: 0.4360, Generator Loss: 4.1471\n",
      "Epoch [0/100], Batch [400/469], Discriminator Loss: 0.0756, Generator Loss: 8.4709\n",
      "Epoch [1/100], Batch [0/469], Discriminator Loss: 0.1129, Generator Loss: 6.5763\n",
      "Epoch [1/100], Batch [100/469], Discriminator Loss: 0.2815, Generator Loss: 3.1278\n",
      "Epoch [1/100], Batch [200/469], Discriminator Loss: 0.3459, Generator Loss: 5.1569\n",
      "Epoch [1/100], Batch [300/469], Discriminator Loss: 0.6497, Generator Loss: 2.8659\n",
      "Epoch [1/100], Batch [400/469], Discriminator Loss: 0.0582, Generator Loss: 5.0110\n",
      "Epoch [2/100], Batch [0/469], Discriminator Loss: 1.0731, Generator Loss: 2.3919\n",
      "Epoch [2/100], Batch [100/469], Discriminator Loss: 0.3548, Generator Loss: 5.0652\n",
      "Epoch [2/100], Batch [200/469], Discriminator Loss: 0.3830, Generator Loss: 5.5832\n",
      "Epoch [2/100], Batch [300/469], Discriminator Loss: 1.8592, Generator Loss: 2.5837\n",
      "Epoch [2/100], Batch [400/469], Discriminator Loss: 1.1590, Generator Loss: 5.1783\n",
      "Epoch [3/100], Batch [0/469], Discriminator Loss: 1.7604, Generator Loss: 2.5175\n",
      "Epoch [3/100], Batch [100/469], Discriminator Loss: 0.8993, Generator Loss: 2.0265\n",
      "Epoch [3/100], Batch [200/469], Discriminator Loss: 0.7824, Generator Loss: 2.7618\n",
      "Epoch [3/100], Batch [300/469], Discriminator Loss: 1.8798, Generator Loss: 0.9994\n",
      "Epoch [3/100], Batch [400/469], Discriminator Loss: 2.2516, Generator Loss: 0.8415\n",
      "Epoch [4/100], Batch [0/469], Discriminator Loss: 0.7808, Generator Loss: 1.5387\n",
      "Epoch [4/100], Batch [100/469], Discriminator Loss: 1.3828, Generator Loss: 1.6774\n",
      "Epoch [4/100], Batch [200/469], Discriminator Loss: 1.8472, Generator Loss: 1.3498\n",
      "Epoch [4/100], Batch [300/469], Discriminator Loss: 1.7679, Generator Loss: 1.5864\n",
      "Epoch [4/100], Batch [400/469], Discriminator Loss: 0.7889, Generator Loss: 1.8576\n",
      "Epoch [5/100], Batch [0/469], Discriminator Loss: 0.3770, Generator Loss: 3.4735\n",
      "Epoch [5/100], Batch [100/469], Discriminator Loss: 0.7488, Generator Loss: 2.7790\n",
      "Epoch [5/100], Batch [200/469], Discriminator Loss: 1.6889, Generator Loss: 2.2914\n",
      "Epoch [5/100], Batch [300/469], Discriminator Loss: 0.6826, Generator Loss: 2.8544\n",
      "Epoch [5/100], Batch [400/469], Discriminator Loss: 1.4246, Generator Loss: 1.5059\n",
      "Epoch [6/100], Batch [0/469], Discriminator Loss: 0.5891, Generator Loss: 2.2157\n",
      "Epoch [6/100], Batch [100/469], Discriminator Loss: 0.5301, Generator Loss: 3.1619\n",
      "Epoch [6/100], Batch [200/469], Discriminator Loss: 0.3065, Generator Loss: 2.5825\n",
      "Epoch [6/100], Batch [300/469], Discriminator Loss: 0.5148, Generator Loss: 2.3236\n",
      "Epoch [6/100], Batch [400/469], Discriminator Loss: 0.3424, Generator Loss: 2.8025\n",
      "Epoch [7/100], Batch [0/469], Discriminator Loss: 0.5831, Generator Loss: 3.0553\n",
      "Epoch [7/100], Batch [100/469], Discriminator Loss: 1.2860, Generator Loss: 1.7913\n",
      "Epoch [7/100], Batch [200/469], Discriminator Loss: 1.7862, Generator Loss: 1.2142\n",
      "Epoch [7/100], Batch [300/469], Discriminator Loss: 0.8035, Generator Loss: 1.9045\n",
      "Epoch [7/100], Batch [400/469], Discriminator Loss: 0.5834, Generator Loss: 2.4631\n",
      "Epoch [8/100], Batch [0/469], Discriminator Loss: 0.5990, Generator Loss: 2.3064\n",
      "Epoch [8/100], Batch [100/469], Discriminator Loss: 0.5418, Generator Loss: 2.6049\n",
      "Epoch [8/100], Batch [200/469], Discriminator Loss: 0.3965, Generator Loss: 2.9934\n",
      "Epoch [8/100], Batch [300/469], Discriminator Loss: 0.5515, Generator Loss: 3.2908\n",
      "Epoch [8/100], Batch [400/469], Discriminator Loss: 0.6098, Generator Loss: 2.8336\n",
      "Epoch [9/100], Batch [0/469], Discriminator Loss: 0.5837, Generator Loss: 2.5130\n",
      "Epoch [9/100], Batch [100/469], Discriminator Loss: 0.2652, Generator Loss: 3.2708\n",
      "Epoch [9/100], Batch [200/469], Discriminator Loss: 0.6500, Generator Loss: 4.1902\n",
      "Epoch [9/100], Batch [300/469], Discriminator Loss: 0.4225, Generator Loss: 2.7275\n",
      "Epoch [9/100], Batch [400/469], Discriminator Loss: 0.4772, Generator Loss: 3.4632\n",
      "Epoch [10/100], Batch [0/469], Discriminator Loss: 0.4030, Generator Loss: 3.3727\n",
      "Epoch [10/100], Batch [100/469], Discriminator Loss: 0.4317, Generator Loss: 3.0803\n",
      "Epoch [10/100], Batch [200/469], Discriminator Loss: 0.2992, Generator Loss: 3.8076\n",
      "Epoch [10/100], Batch [300/469], Discriminator Loss: 0.2418, Generator Loss: 3.9455\n",
      "Epoch [10/100], Batch [400/469], Discriminator Loss: 0.5661, Generator Loss: 2.4331\n",
      "Epoch [11/100], Batch [0/469], Discriminator Loss: 0.3510, Generator Loss: 2.7807\n",
      "Epoch [11/100], Batch [100/469], Discriminator Loss: 0.4656, Generator Loss: 3.0650\n",
      "Epoch [11/100], Batch [200/469], Discriminator Loss: 0.2157, Generator Loss: 4.8684\n",
      "Epoch [11/100], Batch [300/469], Discriminator Loss: 0.2431, Generator Loss: 3.8227\n",
      "Epoch [11/100], Batch [400/469], Discriminator Loss: 0.4106, Generator Loss: 3.2293\n",
      "Epoch [12/100], Batch [0/469], Discriminator Loss: 0.4916, Generator Loss: 3.1103\n",
      "Epoch [12/100], Batch [100/469], Discriminator Loss: 0.4268, Generator Loss: 2.9701\n",
      "Epoch [12/100], Batch [200/469], Discriminator Loss: 0.3967, Generator Loss: 3.0759\n",
      "Epoch [12/100], Batch [300/469], Discriminator Loss: 0.5833, Generator Loss: 2.7211\n",
      "Epoch [12/100], Batch [400/469], Discriminator Loss: 0.7778, Generator Loss: 2.7730\n",
      "Epoch [13/100], Batch [0/469], Discriminator Loss: 0.5610, Generator Loss: 4.5721\n",
      "Epoch [13/100], Batch [100/469], Discriminator Loss: 0.8610, Generator Loss: 3.2399\n",
      "Epoch [13/100], Batch [200/469], Discriminator Loss: 0.5578, Generator Loss: 3.0135\n",
      "Epoch [13/100], Batch [300/469], Discriminator Loss: 0.3756, Generator Loss: 3.8325\n",
      "Epoch [13/100], Batch [400/469], Discriminator Loss: 0.5364, Generator Loss: 3.1979\n",
      "Epoch [14/100], Batch [0/469], Discriminator Loss: 0.6265, Generator Loss: 2.2066\n",
      "Epoch [14/100], Batch [100/469], Discriminator Loss: 0.6799, Generator Loss: 2.3616\n",
      "Epoch [14/100], Batch [200/469], Discriminator Loss: 0.7507, Generator Loss: 2.6048\n",
      "Epoch [14/100], Batch [300/469], Discriminator Loss: 0.3399, Generator Loss: 4.5227\n",
      "Epoch [14/100], Batch [400/469], Discriminator Loss: 0.5541, Generator Loss: 3.5889\n",
      "Epoch [15/100], Batch [0/469], Discriminator Loss: 0.3793, Generator Loss: 3.3451\n",
      "Epoch [15/100], Batch [100/469], Discriminator Loss: 0.5570, Generator Loss: 3.5729\n",
      "Epoch [15/100], Batch [200/469], Discriminator Loss: 0.3743, Generator Loss: 3.2404\n",
      "Epoch [15/100], Batch [300/469], Discriminator Loss: 0.3977, Generator Loss: 3.4298\n",
      "Epoch [15/100], Batch [400/469], Discriminator Loss: 0.5216, Generator Loss: 3.3524\n",
      "Epoch [16/100], Batch [0/469], Discriminator Loss: 0.3687, Generator Loss: 3.5028\n",
      "Epoch [16/100], Batch [100/469], Discriminator Loss: 0.2747, Generator Loss: 3.4285\n",
      "Epoch [16/100], Batch [200/469], Discriminator Loss: 0.3446, Generator Loss: 4.1922\n",
      "Epoch [16/100], Batch [300/469], Discriminator Loss: 0.3301, Generator Loss: 3.3681\n",
      "Epoch [16/100], Batch [400/469], Discriminator Loss: 0.4330, Generator Loss: 3.4223\n",
      "Epoch [17/100], Batch [0/469], Discriminator Loss: 0.6073, Generator Loss: 3.4005\n",
      "Epoch [17/100], Batch [100/469], Discriminator Loss: 0.7589, Generator Loss: 2.5642\n",
      "Epoch [17/100], Batch [200/469], Discriminator Loss: 0.4963, Generator Loss: 4.7872\n",
      "Epoch [17/100], Batch [300/469], Discriminator Loss: 0.6034, Generator Loss: 3.7605\n",
      "Epoch [17/100], Batch [400/469], Discriminator Loss: 0.3815, Generator Loss: 4.1189\n",
      "Epoch [18/100], Batch [0/469], Discriminator Loss: 0.6859, Generator Loss: 2.9337\n",
      "Epoch [18/100], Batch [100/469], Discriminator Loss: 0.5213, Generator Loss: 3.1943\n",
      "Epoch [18/100], Batch [200/469], Discriminator Loss: 0.5842, Generator Loss: 3.8872\n",
      "Epoch [18/100], Batch [300/469], Discriminator Loss: 0.5840, Generator Loss: 2.8462\n",
      "Epoch [18/100], Batch [400/469], Discriminator Loss: 0.3891, Generator Loss: 3.3329\n",
      "Epoch [19/100], Batch [0/469], Discriminator Loss: 1.0179, Generator Loss: 2.1042\n",
      "Epoch [19/100], Batch [100/469], Discriminator Loss: 0.5303, Generator Loss: 2.0299\n",
      "Epoch [19/100], Batch [200/469], Discriminator Loss: 0.4684, Generator Loss: 3.0101\n",
      "Epoch [19/100], Batch [300/469], Discriminator Loss: 0.7025, Generator Loss: 3.2973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/100], Batch [400/469], Discriminator Loss: 0.5578, Generator Loss: 3.9104\n",
      "Epoch [20/100], Batch [0/469], Discriminator Loss: 0.3196, Generator Loss: 2.8157\n",
      "Epoch [20/100], Batch [100/469], Discriminator Loss: 0.3733, Generator Loss: 2.8100\n",
      "Epoch [20/100], Batch [200/469], Discriminator Loss: 0.6121, Generator Loss: 3.4792\n",
      "Epoch [20/100], Batch [300/469], Discriminator Loss: 0.7721, Generator Loss: 3.5488\n",
      "Epoch [20/100], Batch [400/469], Discriminator Loss: 0.4941, Generator Loss: 2.8724\n",
      "Epoch [21/100], Batch [0/469], Discriminator Loss: 0.5055, Generator Loss: 2.5486\n",
      "Epoch [21/100], Batch [100/469], Discriminator Loss: 0.6846, Generator Loss: 2.3681\n",
      "Epoch [21/100], Batch [200/469], Discriminator Loss: 0.5068, Generator Loss: 2.9213\n",
      "Epoch [21/100], Batch [300/469], Discriminator Loss: 0.7492, Generator Loss: 2.5819\n",
      "Epoch [21/100], Batch [400/469], Discriminator Loss: 0.4387, Generator Loss: 3.3490\n",
      "Epoch [22/100], Batch [0/469], Discriminator Loss: 0.6968, Generator Loss: 1.4769\n",
      "Epoch [22/100], Batch [100/469], Discriminator Loss: 0.5383, Generator Loss: 2.6489\n",
      "Epoch [22/100], Batch [200/469], Discriminator Loss: 0.4277, Generator Loss: 3.3865\n",
      "Epoch [22/100], Batch [300/469], Discriminator Loss: 0.5346, Generator Loss: 2.3823\n",
      "Epoch [22/100], Batch [400/469], Discriminator Loss: 0.3793, Generator Loss: 3.4369\n",
      "Epoch [23/100], Batch [0/469], Discriminator Loss: 0.5954, Generator Loss: 2.4420\n",
      "Epoch [23/100], Batch [100/469], Discriminator Loss: 0.4149, Generator Loss: 3.4278\n",
      "Epoch [23/100], Batch [200/469], Discriminator Loss: 0.6611, Generator Loss: 2.8829\n",
      "Epoch [23/100], Batch [300/469], Discriminator Loss: 0.5837, Generator Loss: 3.6411\n",
      "Epoch [23/100], Batch [400/469], Discriminator Loss: 0.8215, Generator Loss: 2.3188\n",
      "Epoch [24/100], Batch [0/469], Discriminator Loss: 0.5469, Generator Loss: 2.8313\n",
      "Epoch [24/100], Batch [100/469], Discriminator Loss: 0.7518, Generator Loss: 3.0703\n",
      "Epoch [24/100], Batch [200/469], Discriminator Loss: 0.6850, Generator Loss: 2.8200\n",
      "Epoch [24/100], Batch [300/469], Discriminator Loss: 0.7224, Generator Loss: 2.1486\n",
      "Epoch [24/100], Batch [400/469], Discriminator Loss: 0.5466, Generator Loss: 2.6919\n",
      "Epoch [25/100], Batch [0/469], Discriminator Loss: 0.7468, Generator Loss: 2.8010\n",
      "Epoch [25/100], Batch [100/469], Discriminator Loss: 0.7070, Generator Loss: 2.3047\n",
      "Epoch [25/100], Batch [200/469], Discriminator Loss: 0.4147, Generator Loss: 4.0574\n",
      "Epoch [25/100], Batch [300/469], Discriminator Loss: 0.5271, Generator Loss: 2.1769\n",
      "Epoch [25/100], Batch [400/469], Discriminator Loss: 0.4532, Generator Loss: 3.1170\n",
      "Epoch [26/100], Batch [0/469], Discriminator Loss: 0.4726, Generator Loss: 2.5543\n",
      "Epoch [26/100], Batch [100/469], Discriminator Loss: 0.3835, Generator Loss: 3.3184\n",
      "Epoch [26/100], Batch [200/469], Discriminator Loss: 0.6244, Generator Loss: 3.2131\n",
      "Epoch [26/100], Batch [300/469], Discriminator Loss: 0.6881, Generator Loss: 2.3530\n",
      "Epoch [26/100], Batch [400/469], Discriminator Loss: 0.8978, Generator Loss: 3.3435\n",
      "Epoch [27/100], Batch [0/469], Discriminator Loss: 0.7757, Generator Loss: 2.4171\n",
      "Epoch [27/100], Batch [100/469], Discriminator Loss: 0.6303, Generator Loss: 2.2374\n",
      "Epoch [27/100], Batch [200/469], Discriminator Loss: 0.7400, Generator Loss: 2.7650\n",
      "Epoch [27/100], Batch [300/469], Discriminator Loss: 0.6113, Generator Loss: 2.4956\n",
      "Epoch [27/100], Batch [400/469], Discriminator Loss: 0.5187, Generator Loss: 3.2759\n",
      "Epoch [28/100], Batch [0/469], Discriminator Loss: 0.7170, Generator Loss: 2.4956\n",
      "Epoch [28/100], Batch [100/469], Discriminator Loss: 0.6968, Generator Loss: 4.1446\n",
      "Epoch [28/100], Batch [200/469], Discriminator Loss: 1.2048, Generator Loss: 1.9856\n",
      "Epoch [28/100], Batch [300/469], Discriminator Loss: 0.7660, Generator Loss: 2.8698\n",
      "Epoch [28/100], Batch [400/469], Discriminator Loss: 0.7081, Generator Loss: 3.4393\n",
      "Epoch [29/100], Batch [0/469], Discriminator Loss: 0.5093, Generator Loss: 2.3860\n",
      "Epoch [29/100], Batch [100/469], Discriminator Loss: 0.7781, Generator Loss: 2.7238\n",
      "Epoch [29/100], Batch [200/469], Discriminator Loss: 0.8510, Generator Loss: 2.5944\n",
      "Epoch [29/100], Batch [300/469], Discriminator Loss: 0.6550, Generator Loss: 2.1767\n",
      "Epoch [29/100], Batch [400/469], Discriminator Loss: 0.6860, Generator Loss: 2.4673\n",
      "Epoch [30/100], Batch [0/469], Discriminator Loss: 0.7208, Generator Loss: 2.4059\n",
      "Epoch [30/100], Batch [100/469], Discriminator Loss: 0.7636, Generator Loss: 2.1484\n",
      "Epoch [30/100], Batch [200/469], Discriminator Loss: 0.7167, Generator Loss: 2.3362\n",
      "Epoch [30/100], Batch [300/469], Discriminator Loss: 0.7886, Generator Loss: 1.7805\n",
      "Epoch [30/100], Batch [400/469], Discriminator Loss: 0.8601, Generator Loss: 2.3049\n",
      "Epoch [31/100], Batch [0/469], Discriminator Loss: 0.7765, Generator Loss: 2.0588\n",
      "Epoch [31/100], Batch [100/469], Discriminator Loss: 0.6284, Generator Loss: 2.0161\n",
      "Epoch [31/100], Batch [200/469], Discriminator Loss: 0.8471, Generator Loss: 2.0511\n",
      "Epoch [31/100], Batch [300/469], Discriminator Loss: 0.7392, Generator Loss: 1.7599\n",
      "Epoch [31/100], Batch [400/469], Discriminator Loss: 0.5430, Generator Loss: 2.2224\n",
      "Epoch [32/100], Batch [0/469], Discriminator Loss: 0.5451, Generator Loss: 2.5499\n",
      "Epoch [32/100], Batch [100/469], Discriminator Loss: 0.7902, Generator Loss: 2.1266\n",
      "Epoch [32/100], Batch [200/469], Discriminator Loss: 0.6614, Generator Loss: 2.3308\n",
      "Epoch [32/100], Batch [300/469], Discriminator Loss: 0.8003, Generator Loss: 2.8199\n",
      "Epoch [32/100], Batch [400/469], Discriminator Loss: 0.6518, Generator Loss: 2.3285\n",
      "Epoch [33/100], Batch [0/469], Discriminator Loss: 0.6436, Generator Loss: 3.2564\n",
      "Epoch [33/100], Batch [100/469], Discriminator Loss: 0.8348, Generator Loss: 2.1888\n",
      "Epoch [33/100], Batch [200/469], Discriminator Loss: 0.8330, Generator Loss: 2.5921\n",
      "Epoch [33/100], Batch [300/469], Discriminator Loss: 1.1649, Generator Loss: 1.7960\n",
      "Epoch [33/100], Batch [400/469], Discriminator Loss: 0.8200, Generator Loss: 2.4785\n",
      "Epoch [34/100], Batch [0/469], Discriminator Loss: 0.5657, Generator Loss: 2.9392\n",
      "Epoch [34/100], Batch [100/469], Discriminator Loss: 0.6167, Generator Loss: 2.6320\n",
      "Epoch [34/100], Batch [200/469], Discriminator Loss: 0.7482, Generator Loss: 2.1819\n"
     ]
    }
   ],
   "source": [
    "# Check if the directory './data/gan/' exists, if not, create it\n",
    "if not os.path.exists('./data/gan/'):\n",
    "    os.makedirs('./data/gan/')\n",
    "\n",
    "start_time = time.time()  # Record the start time of training\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (real_images, _) in enumerate(train_loader):\n",
    "        real_images = real_images.view(-1, image_size).to(device)\n",
    "        batch_size = real_images.shape[0]\n",
    "\n",
    "        # Train Discriminator\n",
    "        discriminator.zero_grad()\n",
    "        real_labels = torch.ones(batch_size, 1).to(device)\n",
    "        fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "        # Real images\n",
    "        real_outputs = discriminator(real_images)\n",
    "        discriminator_real_loss = criterion(real_outputs, real_labels)\n",
    "        discriminator_real_loss.backward()\n",
    "\n",
    "        # Fake images\n",
    "        noise = torch.randn(batch_size, latent_size).to(device)\n",
    "        fake_images = generator(noise)\n",
    "        fake_outputs = discriminator(fake_images.detach())\n",
    "        discriminator_fake_loss = criterion(fake_outputs, fake_labels)\n",
    "        discriminator_fake_loss.backward()\n",
    "\n",
    "        discriminator_loss = discriminator_real_loss + discriminator_fake_loss\n",
    "        discriminator_optimizer.step()\n",
    "\n",
    "        # Train Generator\n",
    "        generator.zero_grad()\n",
    "        fake_outputs = discriminator(fake_images)\n",
    "        generator_loss = criterion(fake_outputs, real_labels)\n",
    "        generator_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "\n",
    "        # Print losses\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{epochs}], Batch [{batch_idx}/{len(train_loader)}], \"\n",
    "                  f\"Discriminator Loss: {discriminator_loss.item():.4f}, \"\n",
    "                  f\"Generator Loss: {generator_loss.item():.4f}\")\n",
    "\n",
    "    # Save generated images\n",
    "    with torch.no_grad():\n",
    "        noise = torch.randn(16, latent_size).to(device)\n",
    "        generated_images = generator(noise).reshape(-1, 1, 28, 28)\n",
    "        save_image(generated_images, f\"./data/gan/gan_generated_images_epoch{epoch + 1}.png\", nrow=4, normalize=True)\n",
    "\n",
    "end_time = time.time()  # Record the end time of training\n",
    "training_time = end_time - start_time  # Calculate the total training time\n",
    "\n",
    "# Convert the training duration to the format hh:mm:ss\n",
    "hours = int(training_time // 3600)\n",
    "minutes = int((training_time % 3600) // 60)\n",
    "seconds = int(training_time % 60)\n",
    "\n",
    "print('Finished Training')\n",
    "print(f\"\\nTraining Time: {hours:02d}:{minutes:02d}:{seconds:02d}\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
