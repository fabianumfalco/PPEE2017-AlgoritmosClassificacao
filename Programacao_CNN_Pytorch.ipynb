{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "192514d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the watermark package.\n",
    "# This package is used to record the versions of other packages used in this Jupyter notebook.\n",
    "# https://github.com/rasbt/watermark\n",
    "!pip install -q -U watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9596fbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc5131fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Fabiano Falcão\n",
      "\n",
      "Website: https://fabianumfalco.github.io/\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.10.6\n",
      "IPython version      : 8.11.0\n",
      "\n",
      "pandas     : 1.5.3\n",
      "torchvision: 0.15.1\n",
      "torch      : 2.0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the watermark extension to display information about the Python version and installed packages.\n",
    "%reload_ext watermark\n",
    "\n",
    "# Display the versions of Python and installed packages.\n",
    "%watermark -a 'Fabiano Falcão' -ws \"https://fabianumfalco.github.io/\" --python --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "720e0ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the PyTorch CUDA library is available on the system.\n",
    "# If it is available, set the device to \"cuda\", indicating that the GPU will be used.\n",
    "# Otherwise, set the device to \"cpu\", indicating that the CPU will be used for computation.\n",
    "# The chosen device will be used to allocate and execute PyTorch tensors.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Display the selected device\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e85f6327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As for work 2, the teacher asked to use the same datasets used in work 1 and compare the results of the two works,\n",
    "# set the device as CPU because I used CPU also for work 1.\n",
    "device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cf294f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net class that defines the architecture of a convolutional neural network (CNN)\n",
    "class NetMNIST(nn.Module):\n",
    "    def __init__(self,dataset='MNIST'):\n",
    "        super(NetMNIST, self).__init__()\n",
    "        # Definition of the layers of the convolutional neural network\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)  # First convolutional layer: input with 1 channel, 6 filters of size 5x5\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # Pooling layer: performs downsampling with a filter of size 2x2 and stride 2\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)  # Second convolutional layer: input with 6 channels, 16 filters of size 5x5\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)  # First fully connected (FC) layer: 16 * 4 * 4 inputs, 120 outputs\n",
    "        self.fc2 = nn.Linear(120, 84)  # Second FC layer: 120 inputs, 84 outputs\n",
    "        if dataset == 'EMNIST':\n",
    "            self.fc3 = nn.Linear(84, 47)  # Third FC layer: 84 inputs, 10 outputs (number of classes of EMNIST)\n",
    "        else:\n",
    "            self.fc3 = nn.Linear(84, 10)  # Third FC layer: 84 inputs, 10 outputs (number of classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward propagation of data through the network layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # Apply the first convolutional layer, followed by a ReLU activation function and pooling\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # Apply the second convolutional layer, followed by a ReLU activation function and pooling\n",
    "        x = x.view(-1, 16 * 4 * 4)  # Reshape the tensor to be compatible with the fully connected layer\n",
    "        x = F.relu(self.fc1(x))  # Apply the first fully connected layer, followed by a ReLU activation function\n",
    "        x = F.relu(self.fc2(x))  # Apply the second fully connected layer, followed by a ReLU activation function\n",
    "        x = self.fc3(x)  # Apply the third fully connected layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d606d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetCIFAR(nn.Module):\n",
    "  def __init__(self, dataset='CIFAR10'):\n",
    "    super(NetCIFAR, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "    self.pool = nn.MaxPool2d(2, 2)\n",
    "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "    self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "    self.fc2 = nn.Linear(120, 84)\n",
    "    if dataset == 'CIFAR100':\n",
    "        self.fc3 = nn.Linear(84, 100)  # Third FC layer: 84 inputs, 100 outputs (number of classes for CIFAR100)\n",
    "    elif dataset == 'CIFAR10':\n",
    "        self.fc3 = nn.Linear(84, 10)  # Third FC layer: 84 inputs, 10 outputs (number of classes for CIFAR10)\n",
    "    else:\n",
    "        raise ValueError('Invalid dataset. Please choose one of the supported datasets CIFAR.')\n",
    "        \n",
    "  def forward(self, x):\n",
    "    x = self.pool(F.relu(self.conv1(x)))\n",
    "    x = self.pool(F.relu(self.conv2(x)))\n",
    "    x = x.view(-1, 16 * 5 * 5)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = self.fc3(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b12cbba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetSTL10(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetSTL10, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)  # Adjusted to handle color images (3 input channels)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 21 * 21, 120)  # Adjusted to match the input size after pooling\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)  # Adjusted to 10 outputs for STL10 dataset\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 21 * 21)  # Adjusted to match the input size after pooling\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f77d63f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createCNN(dataset='MNIST'):\n",
    "    if dataset == 'CIFAR10' or dataset == 'CIFAR100':\n",
    "        net = NetCIFAR(dataset).to(device)\n",
    "    elif dataset == 'STL10':\n",
    "        net = NetSTL10().to(device)\n",
    "    else:\n",
    "        net = NetMNIST(dataset).to(device)\n",
    "    \n",
    "    #print('dataset CNN: ',dataset)\n",
    "    #print(net)\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b4de000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The method responsible for loading the training and test datasets\n",
    "def load_dataset(dataset='MNIST', path='./data', batch_size=32, num_workers=1):\n",
    "    # Define the transformation to be applied to the data\n",
    "    transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),  # Convert images to tensors\n",
    "     transforms.Normalize((0.5,), (0.5,))])  # Normalize images with mean 0.5 and standard deviation 0.5\n",
    "\n",
    "    if dataset == 'MNIST':\n",
    "        # Load the MNIST training dataset\n",
    "        trainset = torchvision.datasets.MNIST(root=path, train=True,\n",
    "                                            download=True, transform=transform)\n",
    "        # Load the MNIST test dataset\n",
    "        testset = torchvision.datasets.MNIST(root=path, train=False,\n",
    "                                        download=True, transform=transform)\n",
    "    elif dataset == 'CIFAR10':\n",
    "        # Load the CIFAR10 training dataset\n",
    "        trainset = torchvision.datasets.CIFAR10(root=path, train=True,\n",
    "                                            download=True, transform=transform)\n",
    "        # Load the CIFAR10 test dataset\n",
    "        testset = torchvision.datasets.CIFAR10(root=path, train=False,\n",
    "                                        download=True, transform=transform)\n",
    "    elif dataset == 'CIFAR100':\n",
    "        # Load the CIFAR100 training dataset\n",
    "        trainset = torchvision.datasets.CIFAR100(root=path, train=True,\n",
    "                                            download=True, transform=transform)\n",
    "        # Load the CIFAR100 test dataset\n",
    "        testset = torchvision.datasets.CIFAR100(root=path, train=False,\n",
    "                                        download=True, transform=transform)\n",
    "    elif dataset == 'FashionMNIST':\n",
    "        # Load the FashionMNIST training dataset\n",
    "        trainset = torchvision.datasets.FashionMNIST(root=path, train=True,\n",
    "                                            download=True, transform=transform)\n",
    "        # Load the FashionMNIST test dataset\n",
    "        testset = torchvision.datasets.FashionMNIST(root=path, train=False,\n",
    "                                        download=True, transform=transform)\n",
    "    elif dataset == 'EMNIST':\n",
    "        # Load the EMNIST training dataset\n",
    "        trainset = torchvision.datasets.EMNIST(root=path, split='balanced', train=True,\n",
    "                                            download=True, transform=transform)\n",
    "        # Load the EMNIST test dataset\n",
    "        testset = torchvision.datasets.EMNIST(root=path, split='balanced', train=False,\n",
    "                                        download=True, transform=transform)\n",
    "    elif dataset == 'KMNIST':\n",
    "        # Load the KMNIST training dataset\n",
    "        trainset = torchvision.datasets.KMNIST(root=path, train=True,\n",
    "                                            download=True, transform=transform)\n",
    "        # Load the KMNIST test dataset\n",
    "        testset = torchvision.datasets.KMNIST(root=path, train=False,\n",
    "                                        download=True, transform=transform)\n",
    "    elif dataset == 'QMNIST':\n",
    "        # Load the QMNIST training dataset\n",
    "        trainset = torchvision.datasets.QMNIST(root=path, what='train',\n",
    "                                            download=True, transform=transform)\n",
    "        # Load the QMNIST test dataset\n",
    "        testset = torchvision.datasets.QMNIST(root=path, what='test',\n",
    "                                        download=True, transform=transform)\n",
    "    elif dataset == 'STL10':\n",
    "        # Load the STL10 training dataset\n",
    "        trainset = torchvision.datasets.STL10(root=path, split='train',\n",
    "                                            download=True, transform=transform)\n",
    "        # Load the STL10 test dataset\n",
    "        testset = torchvision.datasets.STL10(root=path, split='test',\n",
    "                                        download=True, transform=transform)\n",
    "    else:\n",
    "        raise ValueError('Invalid dataset. Please choose one of the supported datasets.')\n",
    "\n",
    "    # Create dataloaders for the training and test datasets\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=num_workers)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    # Return the trainloader and testloader\n",
    "    return trainloader, testloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "297da6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_time_list = []\n",
    "testing_time_list = []\n",
    "accuracy_test_list = []\n",
    "loss_test_list = []\n",
    "epochs = range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c772efd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('[torch-summary] Model Summary with torchvision.datasets.MNIST')\n",
    "#summary(net, (1, 28, 28))  # Resume o modelo, fornecendo o tamanho de entrada (1 canal, 28x28 pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a02928b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(dataset='MNIST'):\n",
    "    \n",
    "    net = createCNN(dataset).to(device)  # Instancia o modelo e o move para a GPU, se disponível\n",
    "    criterion = nn.CrossEntropyLoss().to(device)  # Mova a função de perda para a GPU, se disponível\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)  # Define o otimizador para atualizar os parâmetros do modelo\n",
    "    \n",
    "    \n",
    "    #trainloader, testloader = load_dataset(dataset)  # Load the training and test datasets\n",
    "    trainloader, testloader = load_dataset(dataset)  # Load the training and test datasets\n",
    "\n",
    "\n",
    "    loss_list = []\n",
    "    \n",
    "    print('[%s] %d training images / %d testing images' % (dataset, len(trainloader.dataset),len(testloader.dataset)))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Use tqdm to create a progress bar for the training loop\n",
    "    with tqdm(total=len(trainloader)*len(epochs), unit='batch', ncols=100,desc=f\"Training - {len(epochs):02d} Epochs\") as pbar_training:\n",
    "        for epoch in epochs:\n",
    "        \n",
    "            running_loss = 0.0  # Variable to store the accumulated loss during training\n",
    "\n",
    "        \n",
    "            for i, data in enumerate(trainloader, 0):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)  # Move the data to the GPU, if available\n",
    "\n",
    "                optimizer.zero_grad()  # Zero the gradients of the parameters\n",
    "\n",
    "                outputs = net(inputs)  # Forward pass the data through the model\n",
    "                loss = criterion(outputs, labels)  # Calculate the loss\n",
    "\n",
    "                loss.backward()  # Backpropagation to compute the gradients\n",
    "                optimizer.step()  # Update the model parameters based on the gradients\n",
    "\n",
    "                running_loss += loss.item()  # Accumulate the loss for display purposes\n",
    "\n",
    "                if i == len(trainloader) - 1:\n",
    "                    last_loss = running_loss / ((i % 100) + 1)\n",
    "                    #print('[%d, %5d] Last loss: %.3f' % (epoch + 1, i+1, last_loss))\n",
    "                    loss_list.append(last_loss)  # Store the epoch's average loss in the list\n",
    "\n",
    "                if i % 100 == 99:\n",
    "                    #print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))\n",
    "                    running_loss = 0.0\n",
    "\n",
    "                # Update the tqdm progress bar\n",
    "                pbar_training.update(1)\n",
    "\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "\n",
    "        correct = 0  # Variable to store the number of correct predictions\n",
    "        total = 0  # Variable to store the total number of test examples\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Use tqdm to create a progress bar for the testing loop\n",
    "    with tqdm(total=len(testloader), unit='batch', ncols=100, desc=\"Testing\") as pbar_testing:\n",
    "        with torch.no_grad():\n",
    "            for data in testloader:\n",
    "                images, labels = data\n",
    "                images, labels = images.to(device), labels.to(device)  # Move the data to the GPU, if available\n",
    "\n",
    "                outputs = net(images)  # Forward pass the test data through the model\n",
    "                _, predicted = torch.max(outputs.data, 1)  # Get the predictions with highest probability\n",
    "\n",
    "                total += labels.size(0)  # Update the total number of test examples\n",
    "                correct += (predicted == labels).sum().item()  # Count the number of correct predictions\n",
    "\n",
    "                    # Update the tqdm progress bar\n",
    "                pbar_testing.update(1)        \n",
    "\n",
    "        end_time = time.time()\n",
    "        testing_time = end_time - start_time\n",
    "\n",
    "        accuracy_test = correct / total\n",
    "        loss_test = (total - correct) / total\n",
    "\n",
    "        accuracy_test_pct = 100 * accuracy_test\n",
    "        loss_test_pct = 100 * loss_test\n",
    "\n",
    "    loss_training = loss_list[-1] if loss_list else None  # Get the last element of loss_list or None if empty\n",
    "    \n",
    "    return {\n",
    "        'loss_list': loss_list,\n",
    "        'training_time': training_time,\n",
    "        'testing_time': testing_time,\n",
    "        'loss_training': loss_training,        \n",
    "        'accuracy_test': accuracy_test,\n",
    "        'loss_test': loss_test\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84832955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MNIST] 60000 training images / 10000 testing images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - 10 Epochs: 100%|████████████████████████████████| 18750/18750 [03:21<00:00, 92.99batch/s]\n",
      "Testing: 100%|████████████████████████████████████████████████| 313/313 [00:02<00:00, 120.06batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FashionMNIST] 60000 training images / 10000 testing images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - 10 Epochs: 100%|████████████████████████████████| 18750/18750 [03:25<00:00, 91.36batch/s]\n",
      "Testing: 100%|████████████████████████████████████████████████| 313/313 [00:02<00:00, 119.67batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EMNIST] 112800 training images / 18800 testing images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - 10 Epochs: 100%|████████████████████████████████| 35250/35250 [06:23<00:00, 91.88batch/s]\n",
      "Testing: 100%|████████████████████████████████████████████████| 588/588 [00:04<00:00, 121.79batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KMNIST] 60000 training images / 10000 testing images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - 10 Epochs: 100%|████████████████████████████████| 18750/18750 [03:22<00:00, 92.53batch/s]\n",
      "Testing: 100%|████████████████████████████████████████████████| 313/313 [00:02<00:00, 121.46batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QMNIST] 60000 training images / 60000 testing images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - 10 Epochs: 100%|████████████████████████████████| 18750/18750 [03:22<00:00, 92.57batch/s]\n",
      "Testing: 100%|██████████████████████████████████████████████| 1875/1875 [00:15<00:00, 120.55batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[CIFAR10] 50000 training images / 10000 testing images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - 10 Epochs: 100%|████████████████████████████████| 15630/15630 [03:13<00:00, 80.97batch/s]\n",
      "Testing: 100%|████████████████████████████████████████████████| 313/313 [00:03<00:00, 101.83batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[CIFAR100] 50000 training images / 10000 testing images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - 10 Epochs: 100%|████████████████████████████████| 15630/15630 [03:18<00:00, 78.67batch/s]\n",
      "Testing: 100%|█████████████████████████████████████████████████| 313/313 [00:03<00:00, 86.29batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[STL10] 5000 training images / 8000 testing images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training - 10 Epochs: 100%|██████████████████████████████████| 1570/1570 [01:40<00:00, 15.64batch/s]\n",
      "Testing: 100%|█████████████████████████████████████████████████| 250/250 [00:08<00:00, 30.65batch/s]\n"
     ]
    }
   ],
   "source": [
    "result_MNIST = train_and_test()\n",
    "result_FashionMNIST = train_and_test('FashionMNIST')\n",
    "result_EMNIST = train_and_test('EMNIST')\n",
    "result_KMNIST = train_and_test('KMNIST')\n",
    "result_QMNIST = train_and_test('QMNIST')\n",
    "result_CIFAR10 = train_and_test('CIFAR10')\n",
    "result_CIFAR100 = train_and_test('CIFAR100')\n",
    "result_STL10 = train_and_test('STL10')\n",
    "\n",
    "results = {'MNIST': result_MNIST,\n",
    "           'FashionMNIST': result_FashionMNIST,\n",
    "           'EMNIST':result_EMNIST,\n",
    "           'KMNIST':result_KMNIST,\n",
    "           'QMNIST':result_QMNIST,\n",
    "           'CIFAR10':result_CIFAR10,\n",
    "           'CIFAR100':result_CIFAR100,\n",
    "           'STL10':result_STL10\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "faf94ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Loss MNIST</th>\n",
       "      <th>Loss FashionMNIST</th>\n",
       "      <th>Loss EMNIST</th>\n",
       "      <th>Loss KMNIST</th>\n",
       "      <th>Loss QMNIST</th>\n",
       "      <th>Loss CIFAR10</th>\n",
       "      <th>Loss CIFAR100</th>\n",
       "      <th>Loss STL10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.089726</td>\n",
       "      <td>0.416776</td>\n",
       "      <td>0.519638</td>\n",
       "      <td>0.178631</td>\n",
       "      <td>0.094273</td>\n",
       "      <td>1.422449</td>\n",
       "      <td>3.577976</td>\n",
       "      <td>1.660716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.047583</td>\n",
       "      <td>0.356508</td>\n",
       "      <td>0.431161</td>\n",
       "      <td>0.105822</td>\n",
       "      <td>0.049328</td>\n",
       "      <td>1.292976</td>\n",
       "      <td>3.319002</td>\n",
       "      <td>1.478024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.048912</td>\n",
       "      <td>0.296615</td>\n",
       "      <td>0.408110</td>\n",
       "      <td>0.068046</td>\n",
       "      <td>0.047058</td>\n",
       "      <td>1.149686</td>\n",
       "      <td>3.141521</td>\n",
       "      <td>1.210577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.047993</td>\n",
       "      <td>0.323669</td>\n",
       "      <td>0.403290</td>\n",
       "      <td>0.062656</td>\n",
       "      <td>0.042161</td>\n",
       "      <td>1.057990</td>\n",
       "      <td>3.033691</td>\n",
       "      <td>1.070730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.039184</td>\n",
       "      <td>0.276312</td>\n",
       "      <td>0.335292</td>\n",
       "      <td>0.048570</td>\n",
       "      <td>0.032428</td>\n",
       "      <td>1.022587</td>\n",
       "      <td>2.909626</td>\n",
       "      <td>0.888005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.022122</td>\n",
       "      <td>0.261107</td>\n",
       "      <td>0.348213</td>\n",
       "      <td>0.055086</td>\n",
       "      <td>0.025067</td>\n",
       "      <td>0.973795</td>\n",
       "      <td>2.897114</td>\n",
       "      <td>0.746448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.014859</td>\n",
       "      <td>0.246660</td>\n",
       "      <td>0.397106</td>\n",
       "      <td>0.053482</td>\n",
       "      <td>0.025561</td>\n",
       "      <td>0.948463</td>\n",
       "      <td>2.794874</td>\n",
       "      <td>0.562957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.021356</td>\n",
       "      <td>0.230851</td>\n",
       "      <td>0.269430</td>\n",
       "      <td>0.045760</td>\n",
       "      <td>0.016502</td>\n",
       "      <td>0.873080</td>\n",
       "      <td>2.755467</td>\n",
       "      <td>0.395778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.010727</td>\n",
       "      <td>0.218875</td>\n",
       "      <td>0.296457</td>\n",
       "      <td>0.039353</td>\n",
       "      <td>0.013984</td>\n",
       "      <td>0.938096</td>\n",
       "      <td>2.706670</td>\n",
       "      <td>0.258475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.025281</td>\n",
       "      <td>0.218843</td>\n",
       "      <td>0.298862</td>\n",
       "      <td>0.030447</td>\n",
       "      <td>0.015559</td>\n",
       "      <td>0.843771</td>\n",
       "      <td>2.682704</td>\n",
       "      <td>0.162463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Loss MNIST  Loss FashionMNIST  Loss EMNIST  Loss KMNIST  Loss QMNIST  \\\n",
       "Epoch                                                                         \n",
       "1        0.089726           0.416776     0.519638     0.178631     0.094273   \n",
       "2        0.047583           0.356508     0.431161     0.105822     0.049328   \n",
       "3        0.048912           0.296615     0.408110     0.068046     0.047058   \n",
       "4        0.047993           0.323669     0.403290     0.062656     0.042161   \n",
       "5        0.039184           0.276312     0.335292     0.048570     0.032428   \n",
       "6        0.022122           0.261107     0.348213     0.055086     0.025067   \n",
       "7        0.014859           0.246660     0.397106     0.053482     0.025561   \n",
       "8        0.021356           0.230851     0.269430     0.045760     0.016502   \n",
       "9        0.010727           0.218875     0.296457     0.039353     0.013984   \n",
       "10       0.025281           0.218843     0.298862     0.030447     0.015559   \n",
       "\n",
       "       Loss CIFAR10  Loss CIFAR100  Loss STL10  \n",
       "Epoch                                           \n",
       "1          1.422449       3.577976    1.660716  \n",
       "2          1.292976       3.319002    1.478024  \n",
       "3          1.149686       3.141521    1.210577  \n",
       "4          1.057990       3.033691    1.070730  \n",
       "5          1.022587       2.909626    0.888005  \n",
       "6          0.973795       2.897114    0.746448  \n",
       "7          0.948463       2.794874    0.562957  \n",
       "8          0.873080       2.755467    0.395778  \n",
       "9          0.938096       2.706670    0.258475  \n",
       "10         0.843771       2.682704    0.162463  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "epoch_list = [x + 1 for x in list(epochs)]\n",
    "\n",
    "table_data = {'Epoch': epoch_list}\n",
    "\n",
    "for dataset, result in results.items():\n",
    "    loss_list = result['loss_list']\n",
    "    column_name = 'Loss ' + dataset\n",
    "    table_data[column_name] = loss_list\n",
    "\n",
    "df_training = pd.DataFrame(table_data)\n",
    "df_training.set_index('Epoch', inplace=True)\n",
    "df_training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17583cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Testing Time</th>\n",
       "      <th>Loss Training</th>\n",
       "      <th>Loss Test</th>\n",
       "      <th>Accuracy Test</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MNIST</th>\n",
       "      <td>[201.64672827720642]</td>\n",
       "      <td>[2.611858367919922]</td>\n",
       "      <td>[0.025280748233817575]</td>\n",
       "      <td>[0.012]</td>\n",
       "      <td>[0.988]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FashionMNIST</th>\n",
       "      <td>[205.22526931762695]</td>\n",
       "      <td>[2.6216888427734375]</td>\n",
       "      <td>[0.2188428450624148]</td>\n",
       "      <td>[0.1015]</td>\n",
       "      <td>[0.8985]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EMNIST</th>\n",
       "      <td>[383.6528193950653]</td>\n",
       "      <td>[4.833750486373901]</td>\n",
       "      <td>[0.29886192589998245]</td>\n",
       "      <td>[0.13632978723404254]</td>\n",
       "      <td>[0.8636702127659575]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KMNIST</th>\n",
       "      <td>[202.64346361160278]</td>\n",
       "      <td>[2.5809340476989746]</td>\n",
       "      <td>[0.030447382162092255]</td>\n",
       "      <td>[0.0682]</td>\n",
       "      <td>[0.9318]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QMNIST</th>\n",
       "      <td>[202.55193328857422]</td>\n",
       "      <td>[15.558828353881836]</td>\n",
       "      <td>[0.015559190554522501]</td>\n",
       "      <td>[0.012516666666666667]</td>\n",
       "      <td>[0.9874833333333334]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CIFAR10</th>\n",
       "      <td>[193.03821063041687]</td>\n",
       "      <td>[3.0755178928375244]</td>\n",
       "      <td>[0.843771411312951]</td>\n",
       "      <td>[0.3582]</td>\n",
       "      <td>[0.6418]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CIFAR100</th>\n",
       "      <td>[198.67870664596558]</td>\n",
       "      <td>[3.6324987411499023]</td>\n",
       "      <td>[2.6827038526535034]</td>\n",
       "      <td>[0.715]</td>\n",
       "      <td>[0.285]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STL10</th>\n",
       "      <td>[100.36720490455627]</td>\n",
       "      <td>[8.163002252578735]</td>\n",
       "      <td>[0.16246325457305238]</td>\n",
       "      <td>[0.488875]</td>\n",
       "      <td>[0.511125]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Training Time          Testing Time  \\\n",
       "Dataset                                                    \n",
       "MNIST         [201.64672827720642]   [2.611858367919922]   \n",
       "FashionMNIST  [205.22526931762695]  [2.6216888427734375]   \n",
       "EMNIST         [383.6528193950653]   [4.833750486373901]   \n",
       "KMNIST        [202.64346361160278]  [2.5809340476989746]   \n",
       "QMNIST        [202.55193328857422]  [15.558828353881836]   \n",
       "CIFAR10       [193.03821063041687]  [3.0755178928375244]   \n",
       "CIFAR100      [198.67870664596558]  [3.6324987411499023]   \n",
       "STL10         [100.36720490455627]   [8.163002252578735]   \n",
       "\n",
       "                       Loss Training               Loss Test  \\\n",
       "Dataset                                                        \n",
       "MNIST         [0.025280748233817575]                 [0.012]   \n",
       "FashionMNIST    [0.2188428450624148]                [0.1015]   \n",
       "EMNIST         [0.29886192589998245]   [0.13632978723404254]   \n",
       "KMNIST        [0.030447382162092255]                [0.0682]   \n",
       "QMNIST        [0.015559190554522501]  [0.012516666666666667]   \n",
       "CIFAR10          [0.843771411312951]                [0.3582]   \n",
       "CIFAR100        [2.6827038526535034]                 [0.715]   \n",
       "STL10          [0.16246325457305238]              [0.488875]   \n",
       "\n",
       "                     Accuracy Test  \n",
       "Dataset                             \n",
       "MNIST                      [0.988]  \n",
       "FashionMNIST              [0.8985]  \n",
       "EMNIST        [0.8636702127659575]  \n",
       "KMNIST                    [0.9318]  \n",
       "QMNIST        [0.9874833333333334]  \n",
       "CIFAR10                   [0.6418]  \n",
       "CIFAR100                   [0.285]  \n",
       "STL10                   [0.511125]  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_list = []\n",
    "accuracy_test_list = []\n",
    "loss_test_list = []\n",
    "loss_training_list = []\n",
    "training_time_list = []\n",
    "testing_time_list = []\n",
    "\n",
    "# Percorrer o dicionário results e extrair os valores\n",
    "for dataset, result in results.items():\n",
    "    dataset_list.append(dataset)  # Adicionar o nome do dataset à lista\n",
    "    \n",
    "    accuracy_test = result.get('accuracy_test', None)\n",
    "    if isinstance(accuracy_test, float):  # Verificar se é um número float\n",
    "        accuracy_test = [accuracy_test]  # Converter para lista\n",
    "    accuracy_test_list.append(accuracy_test)\n",
    "    \n",
    "    loss_test = result.get('loss_test', None)\n",
    "    if isinstance(loss_test, float):  # Verificar se é um número float\n",
    "        loss_test = [loss_test]  # Converter para lista\n",
    "    loss_test_list.append(loss_test)\n",
    "    \n",
    "    loss_training = result.get('loss_training', None)\n",
    "    if isinstance(loss_training, float):  # Verificar se é um número float\n",
    "        loss_training = [loss_training]  # Converter para lista\n",
    "    loss_training_list.append(loss_training)    \n",
    "    \n",
    "    training_time = result.get('training_time', None)\n",
    "    if isinstance(training_time, float):  # Verificar se é um número float\n",
    "        training_time = [training_time]  # Converter para lista\n",
    "    training_time_list.append(training_time)\n",
    "    \n",
    "    testing_time = result.get('testing_time', None)\n",
    "    if isinstance(testing_time, float):  # Verificar se é um número float\n",
    "        testing_time = [testing_time]  # Converter para lista\n",
    "    testing_time_list.append(testing_time)\n",
    "\n",
    "# Create a dictionary with the list values\n",
    "data = {'Dataset': dataset_list,\n",
    "        'Training Time': training_time_list,\n",
    "        'Testing Time': testing_time_list,\n",
    "        'Loss Training': loss_training_list,\n",
    "        'Loss Test': loss_test_list,\n",
    "        'Accuracy Test': accuracy_test_list}\n",
    "\n",
    "# Create a DataFrame from the dictionary\n",
    "df_model_dataset = pd.DataFrame(data)\n",
    "\n",
    "df_model_dataset.set_index('Dataset', inplace=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "#df_model_dataset.transpose()\n",
    "df_model_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db12f027",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
