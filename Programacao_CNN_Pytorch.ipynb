{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "192514d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instala o pacote watermark. \n",
    "# Esse pacote é usado para gravar as versões de outros pacotes usados neste jupyter notebook.\n",
    "# https://github.com/rasbt/watermark\n",
    "!pip install -q -U watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9596fbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importa várias bibliotecas do PyTorch necessárias para criar e treinar redes neurais\n",
    "import torch  # Importa o módulo principal do PyTorch\n",
    "import torchvision  # Importa o módulo torchvision para trabalhar com conjuntos de dados populares e modelos de redes neurais pré-treinados\n",
    "import torchvision.transforms as transforms  # Importa o módulo torchvision.transforms para realizar transformações nos dados\n",
    "import torch.nn as nn  # Importa o módulo nn do PyTorch para criar redes neurais\n",
    "import torch.nn.functional as F  # Importa o módulo functional do PyTorch para utilizar funções de ativação e outras operações\n",
    "import torch.optim as optim  # Importa o módulo optim do PyTorch para otimização de modelos de redes neurais\n",
    "from torchsummary import summary  # Importa a função summary para apresentar o resumo do modelo\n",
    "\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc5131fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.10.6\n",
      "IPython version      : 8.11.0\n",
      "\n",
      "torch      : 2.0.0\n",
      "pandas     : 1.5.3\n",
      "torchvision: 0.15.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Carrega a extensão `watermark` para exibir informações sobre a versão do Python e dos pacotes instalados.\n",
    "%reload_ext watermark\n",
    "\n",
    "# Exibe as versões do Python e dos pacotes instalados.\n",
    "%watermark --python --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "720e0ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verifica se a biblioteca CUDA do PyTorch está disponível no sistema.\n",
    "# Se estiver disponível, o dispositivo é definido como \"cuda\", indicando que a GPU será utilizada.\n",
    "# Caso contrário, o dispositivo é definido como \"cpu\", indicando que a CPU será utilizada para a computação.\n",
    "# O dispositivo escolhido será usado para alocar e executar os tensores do PyTorch.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Exibe o dispositivo selecionado\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cf294f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe Net que define a arquitetura de uma rede neural convolucional (CNN)\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Definição das camadas da rede neural convolucional\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)  # Primeira camada de convolução: entrada com 1 canal, 6 filtros de tamanho 5x5\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # Camada de pooling: realiza o downsampling com um filtro de tamanho 2x2 e stride 2\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)  # Segunda camada de convolução: entrada com 6 canais, 16 filtros de tamanho 5x5\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)  # Primeira camada fully connected (FC): 16 * 4 * 4 entradas, 120 saídas\n",
    "        self.fc2 = nn.Linear(120, 84)  # Segunda camada FC: 120 entradas, 84 saídas\n",
    "        self.fc3 = nn.Linear(84, 10)  # Terceira camada FC: 84 entradas, 10 saídas (número de classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Propagação dos dados através das camadas da rede\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # Aplica a primeira camada de convolução, seguida de uma função de ativação ReLU e pooling\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # Aplica a segunda camada de convolução, seguida de uma função de ativação ReLU e pooling\n",
    "        x = x.view(-1, 16 * 4 * 4)  # Redimensiona o tensor para ser compatível com a camada fully connected\n",
    "        x = F.relu(self.fc1(x))  # Aplica a primeira camada fully connected, seguida de uma função de ativação ReLU\n",
    "        x = F.relu(self.fc2(x))  # Aplica a segunda camada fully connected, seguida de uma função de ativação ReLU\n",
    "        x = self.fc3(x)  # Aplica a terceira camada fully connected\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b4de000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# O método responsável por carregar os conjuntos de dados de treinamento e teste \n",
    "def load_dataset(dataset='MNIST', path='./data', batch_size=32, num_workers=1):\n",
    "    # Define a transformação a ser aplicada aos dados\n",
    "    transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),  # Converte as imagens em tensores\n",
    "     transforms.Normalize((0.5,), (0.5,))])  # Normaliza as imagens com média 0.5 e desvio padrão 0.5\n",
    "    \n",
    "    if dataset == 'MNIST':\n",
    "        # Carrega o conjunto de dados de treinamento do MNIST\n",
    "        trainset = torchvision.datasets.MNIST(root=path, train=True,\n",
    "                                              download=True, transform=transform)\n",
    "        \n",
    "        # Carrega o conjunto de dados de teste do MNIST\n",
    "        testset = torchvision.datasets.MNIST(root=path, train=False,\n",
    "                                             download=True, transform=transform)\n",
    "    elif dataset == 'FashionMNIST':\n",
    "        # Carrega o conjunto de dados de treinamento do FashionMNIST\n",
    "        trainset = torchvision.datasets.FashionMNIST(root=path, train=True,\n",
    "                                                     download=True, transform=transform)\n",
    "        \n",
    "        # Carrega o conjunto de dados de teste do FashionMNIST\n",
    "        testset = torchvision.datasets.FashionMNIST(root=path, train=False,\n",
    "                                                    download=True, transform=transform)\n",
    "    else:\n",
    "        raise ValueError('Dataset not supported. Please choose either \"MNIST\" or \"FashionMNIST\".')\n",
    "    \n",
    "    # Cria um dataloader para o conjunto de dados de treinamento\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                              shuffle=True, num_workers=num_workers)\n",
    "\n",
    "    # Cria um dataloader para o conjunto de dados de teste\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                             shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    # Retorna os dataloaders do conjunto de treinamento e teste\n",
    "    return trainloader, testloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "297da6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_time_list = []\n",
    "testing_time_list = []\n",
    "accuracy_test_list = []\n",
    "loss_test_list = []\n",
    "epochs = range(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d89ffbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().to(device)  # Instancia o modelo e o move para a GPU, se disponível\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)  # Mova a função de perda para a GPU, se disponível\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)  # Define o otimizador para atualizar os parâmetros do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c772efd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch-summary] Model Summary with torchvision.datasets.MNIST\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 6, 24, 24]             156\n",
      "         MaxPool2d-2            [-1, 6, 12, 12]               0\n",
      "            Conv2d-3             [-1, 16, 8, 8]           2,416\n",
      "         MaxPool2d-4             [-1, 16, 4, 4]               0\n",
      "            Linear-5                  [-1, 120]          30,840\n",
      "            Linear-6                   [-1, 84]          10,164\n",
      "            Linear-7                   [-1, 10]             850\n",
      "================================================================\n",
      "Total params: 44,426\n",
      "Trainable params: 44,426\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.04\n",
      "Params size (MB): 0.17\n",
      "Estimated Total Size (MB): 0.22\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('[torch-summary] Model Summary with torchvision.datasets.MNIST')\n",
    "summary(net, (1, 28, 28))  # Resume o modelo, fornecendo o tamanho de entrada (1 canal, 28x28 pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92deec2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with 60000 images\n",
      "[1,   100] loss: 1.325\n",
      "[1,   200] loss: 0.412\n",
      "[1,   300] loss: 0.264\n",
      "[1,   400] loss: 0.221\n",
      "[1,   500] loss: 0.179\n",
      "[1,   600] loss: 0.154\n",
      "[1,   700] loss: 0.143\n",
      "[1,   800] loss: 0.134\n",
      "[1,   900] loss: 0.103\n",
      "[1,  1000] loss: 0.120\n",
      "[1,  1100] loss: 0.103\n",
      "[1,  1200] loss: 0.109\n",
      "[1,  1300] loss: 0.102\n",
      "[1,  1400] loss: 0.104\n",
      "[1,  1500] loss: 0.097\n",
      "[1,  1600] loss: 0.076\n",
      "[1,  1700] loss: 0.076\n",
      "[1,  1800] loss: 0.066\n",
      "[1,  1875] Last loss: 0.081\n",
      "[2,   100] loss: 0.072\n",
      "[2,   200] loss: 0.080\n",
      "[2,   300] loss: 0.062\n",
      "[2,   400] loss: 0.072\n",
      "[2,   500] loss: 0.055\n",
      "[2,   600] loss: 0.073\n",
      "[2,   700] loss: 0.061\n",
      "[2,   800] loss: 0.064\n",
      "[2,   900] loss: 0.048\n",
      "[2,  1000] loss: 0.060\n",
      "[2,  1100] loss: 0.064\n",
      "[2,  1200] loss: 0.060\n",
      "[2,  1300] loss: 0.071\n",
      "[2,  1400] loss: 0.062\n",
      "[2,  1500] loss: 0.074\n",
      "[2,  1600] loss: 0.052\n",
      "[2,  1700] loss: 0.053\n",
      "[2,  1800] loss: 0.065\n",
      "[2,  1875] Last loss: 0.050\n",
      "Finished Training\n",
      "\n",
      "Training Time: 00:00:26\n",
      "Training Time Duration:  26.94065833091736\n",
      "\n",
      "Testing Time: 00:00:02\n",
      "Testing Time Duration:  2.4079246520996094\n",
      "\n",
      "Accuracy of the network on the 10000 test images: 0.980\n",
      "Loss of the network on the 10000 test images: 0.020\n",
      "\n",
      "Accuracy of the network on the 10000 test images: 98.000 %\n",
      "Loss of the network on the 10000 test images: 2.000 %\n"
     ]
    }
   ],
   "source": [
    "trainloader, testloader = load_dataset()  # Carrega os conjuntos de dados de treinamento e teste\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "print('Starting training with %d images' % ( len(trainloader.dataset)))\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in epochs:\n",
    "    running_loss = 0.0  # Variável para armazenar a perda acumulada durante o treinamento\n",
    "    \n",
    "   \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Mova os dados para a GPU, se disponível\n",
    "\n",
    "        optimizer.zero_grad()  # Zera os gradientes dos parâmetros\n",
    "\n",
    "        outputs = net(inputs)  # Propagação direta dos dados através do modelo\n",
    "        loss = criterion(outputs, labels)  # Calcula a perda\n",
    "\n",
    "        loss.backward()  # Retropropagação para calcular os gradientes\n",
    "        optimizer.step()  # Atualiza os parâmetros do modelo com base nos gradientes\n",
    "\n",
    "        running_loss += loss.item()  # Acumula a perda para fins de exibição\n",
    "            \n",
    "        if i == len(trainloader) - 1:\n",
    "            last_loss = running_loss /  ((i % 100)+1)\n",
    "            print('[%d, %5d] Last loss: %.3f' % (epoch + 1, i+1, last_loss  ) )\n",
    "            loss_list.append(last_loss )  # Armazena o loss médio da época na lista\n",
    "\n",
    "        if i % 100 == 99:\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0       \n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "training_time_list.append(duration)\n",
    "# Converte a duração para o formato hh:mm:ss\n",
    "hours = int(duration // 3600)\n",
    "minutes = int((duration % 3600) // 60)\n",
    "seconds = int(duration % 60)\n",
    "\n",
    "print('Finished Training')\n",
    "print(f\"\\nTraining Time: {hours:02d}:{minutes:02d}:{seconds:02d}\")\n",
    "print('Training Time Duration: ',duration)\n",
    "\n",
    "\n",
    "correct = 0  # Variável para armazenar o número de previsões corretas\n",
    "total = 0  # Variável para armazenar o número total de exemplos de teste\n",
    "\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)  # Mova os dados para a GPU, se disponível\n",
    "\n",
    "        outputs = net(images)  # Propagação direta dos dados de teste através do modelo\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Obtém as previsões com maior probabilidade\n",
    "\n",
    "        total += labels.size(0)  # Atualiza o número total de exemplos de teste\n",
    "        correct += (predicted == labels).sum().item()  # Conta o número de previsões corretas\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "testing_time_list.append(duration)        \n",
    "        \n",
    "accuracy_test = correct / total\n",
    "loss_test = (total - correct) / total\n",
    "        \n",
    "accuracy_test_pct = 100 * accuracy_test\n",
    "loss_test_pct = 100 * loss_test\n",
    "\n",
    "accuracy_test_list.append(accuracy_test)\n",
    "loss_test_list.append(loss_test)\n",
    "\n",
    "# Converte a duração para o formato hh:mm:ss\n",
    "hours = int(duration // 3600)\n",
    "minutes = int((duration % 3600) // 60)\n",
    "seconds = int(duration % 60)\n",
    "print(f\"\\nTesting Time: {hours:02d}:{minutes:02d}:{seconds:02d}\")\n",
    "print('Testing Time Duration: ',duration)\n",
    "\n",
    "print('\\nAccuracy of the network on the %d test images: %.3f' % (len(testloader.dataset), accuracy_test))\n",
    "print('Loss of the network on the %d test images: %.3f' % (len(testloader.dataset), loss_test))\n",
    "print('\\nAccuracy of the network on the %d test images: %.3f %%' % (len(testloader.dataset), accuracy_test_pct))\n",
    "print('Loss of the network on the %d test images: %.3f %%' % (total, loss_test_pct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28427135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss_NMIST</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.081255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.049743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss_NMIST\n",
       "epoch            \n",
       "1        0.081255\n",
       "2        0.049743"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_list = [x + 1 for x in list(epochs)]\n",
    "\n",
    "table_data = {'epoch': epoch_list, 'loss_NMIST': loss_list}\n",
    "df_training = pd.DataFrame(table_data)\n",
    "df_training.set_index('epoch', inplace=True)\n",
    "df_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72bf60ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Dataset</th>\n",
       "      <th>NMIST</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Training Time (s)</th>\n",
       "      <td>27.287401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Testing Time (s)</th>\n",
       "      <td>2.291809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy Test</th>\n",
       "      <td>0.988300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loss Test</th>\n",
       "      <td>0.011700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Dataset                NMIST\n",
       "Training Time (s)  27.287401\n",
       "Testing Time (s)    2.291809\n",
       "Accuracy Test       0.988300\n",
       "Loss Test           0.011700"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually create the dataset_list\n",
    "dataset_list = [\"NMIST\"]\n",
    "#dataset_list = [\"NMIST\",\"CIFAR10\",\"CIFAR100\",\"FashionMNIST\",\"EMNIST\",\"KMNIST\",\"QMNIST\",\"STL10\",\"USPS\"] \n",
    "\n",
    "# Create a dictionary with the list values\n",
    "data = {'Dataset': dataset_list,\n",
    "        'Training Time (s)': training_time_list,\n",
    "        'Testing Time (s)': testing_time_list,\n",
    "        'Accuracy Test': accuracy_test_list,\n",
    "        'Loss Test': loss_test_list}\n",
    "\n",
    "# Create a DataFrame from the dictionary\n",
    "df_model_dataset = pd.DataFrame(data)\n",
    "\n",
    "df_model_dataset.set_index('Dataset', inplace=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "df_model_dataset.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a96c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
