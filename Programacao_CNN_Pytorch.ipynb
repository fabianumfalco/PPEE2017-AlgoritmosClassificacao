{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "192514d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the watermark package.\n",
    "# This package is used to record the versions of other packages used in this Jupyter notebook.\n",
    "# https://github.com/rasbt/watermark\n",
    "!pip install -q -U watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9596fbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc5131fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Fabiano Falcão\n",
      "\n",
      "Website: https://fabianumfalco.github.io/\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.10.6\n",
      "IPython version      : 8.11.0\n",
      "\n",
      "torchvision: 0.15.1\n",
      "torch      : 2.0.0\n",
      "pandas     : 1.5.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the watermark extension to display information about the Python version and installed packages.\n",
    "%reload_ext watermark\n",
    "\n",
    "# Display the versions of Python and installed packages.\n",
    "%watermark -a 'Fabiano Falcão' -ws \"https://fabianumfalco.github.io/\" --python --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "720e0ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if the PyTorch CUDA library is available on the system.\n",
    "# If it is available, set the device to \"cuda\", indicating that the GPU will be used.\n",
    "# Otherwise, set the device to \"cpu\", indicating that the CPU will be used for computation.\n",
    "# The chosen device will be used to allocate and execute PyTorch tensors.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Display the selected device\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cf294f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net class that defines the architecture of a convolutional neural network (CNN)\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Definition of the layers of the convolutional neural network\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)  # First convolutional layer: input with 1 channel, 6 filters of size 5x5\n",
    "        self.pool = nn.MaxPool2d(2, 2)  # Pooling layer: performs downsampling with a filter of size 2x2 and stride 2\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)  # Second convolutional layer: input with 6 channels, 16 filters of size 5x5\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)  # First fully connected (FC) layer: 16 * 4 * 4 inputs, 120 outputs\n",
    "        self.fc2 = nn.Linear(120, 84)  # Second FC layer: 120 inputs, 84 outputs\n",
    "        self.fc3 = nn.Linear(84, 10)  # Third FC layer: 84 inputs, 10 outputs (number of classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward propagation of data through the network layers\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # Apply the first convolutional layer, followed by a ReLU activation function and pooling\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # Apply the second convolutional layer, followed by a ReLU activation function and pooling\n",
    "        x = x.view(-1, 16 * 4 * 4)  # Reshape the tensor to be compatible with the fully connected layer\n",
    "        x = F.relu(self.fc1(x))  # Apply the first fully connected layer, followed by a ReLU activation function\n",
    "        x = F.relu(self.fc2(x))  # Apply the second fully connected layer, followed by a ReLU activation function\n",
    "        x = self.fc3(x)  # Apply the third fully connected layer\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b4de000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The method responsible for loading the training and test datasets\n",
    "def load_dataset(dataset='MNIST', path='./data', batch_size=32, num_workers=1):\n",
    "    # Define the transformation to be applied to the data\n",
    "    transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),  # Convert images to tensors\n",
    "     transforms.Normalize((0.5,), (0.5,))])  # Normalize images with mean 0.5 and standard deviation 0.5\n",
    "\n",
    "    if dataset == 'MNIST':\n",
    "        # Load the MNIST training dataset\n",
    "        trainset = torchvision.datasets.MNIST(root=path, train=True,\n",
    "                                            download=True, transform=transform)\n",
    "        # Load the MNIST test dataset\n",
    "        testset = torchvision.datasets.MNIST(root=path, train=False,\n",
    "                                        download=True, transform=transform)\n",
    "    elif dataset == 'CIFAR10':\n",
    "        # Load the CIFAR10 training dataset\n",
    "        trainset = torchvision.datasets.CIFAR10(root=path, train=True,\n",
    "                                            download=True, transform=transform)\n",
    "        # Load the CIFAR10 test dataset\n",
    "        testset = torchvision.datasets.CIFAR10(root=path, train=False,\n",
    "                                        download=True, transform=transform)\n",
    "    elif dataset == 'CIFAR100':\n",
    "        # Load the CIFAR100 training dataset\n",
    "        trainset = torchvision.datasets.CIFAR100(root=path, train=True,\n",
    "                                            download=True, transform=transform)\n",
    "        # Load the CIFAR100 test dataset\n",
    "        testset = torchvision.datasets.CIFAR100(root=path, train=False,\n",
    "                                        download=True, transform=transform)\n",
    "    elif dataset == 'FashionMNIST':\n",
    "        # Load the FashionMNIST training dataset\n",
    "        trainset = torchvision.datasets.FashionMNIST(root=path, train=True,\n",
    "                                            download=True, transform=transform)\n",
    "        # Load the FashionMNIST test dataset\n",
    "        testset = torchvision.datasets.FashionMNIST(root=path, train=False,\n",
    "                                        download=True, transform=transform)\n",
    "    elif dataset == 'EMNIST':\n",
    "        # Load the EMNIST training dataset\n",
    "        trainset = torchvision.datasets.EMNIST(root=path, split='balanced', train=True,\n",
    "                                            download=True, transform=transform)\n",
    "        # Load the EMNIST test dataset\n",
    "        testset = torchvision.datasets.EMNIST(root=path, split='balanced', train=False,\n",
    "                                        download=True, transform=transform)\n",
    "    elif dataset == 'KMNIST':\n",
    "        # Load the KMNIST training dataset\n",
    "        trainset = torchvision.datasets.KMNIST(root=path, train=True,\n",
    "                                            download=True, transform=transform)\n",
    "        # Load the KMNIST test dataset\n",
    "        testset = torchvision.datasets.KMNIST(root=path, train=False,\n",
    "                                        download=True, transform=transform)\n",
    "    elif dataset == 'QMNIST':\n",
    "        # Load the QMNIST training dataset\n",
    "        trainset = torchvision.datasets.QMNIST(root=path, what='train',\n",
    "                                            download=True, transform=transform)\n",
    "        # Load the QMNIST test dataset\n",
    "        testset = torchvision.datasets.QMNIST(root=path, what='test',\n",
    "                                        download=True, transform=transform)\n",
    "    elif dataset == 'STL10':\n",
    "        # Load the STL10 training dataset\n",
    "        trainset = torchvision.datasets.STL10(root=path, split='train',\n",
    "                                            download=True, transform=transform)\n",
    "        # Load the STL10 test dataset\n",
    "        testset = torchvision.datasets.STL10(root=path, split='test',\n",
    "                                        download=True, transform=transform)\n",
    "    elif dataset == 'USPS':\n",
    "        # Load the USPS training dataset\n",
    "        trainset = torchvision.datasets.USPS(root=path, train=True,\n",
    "                                            download=True, transform=transform)\n",
    "        # Load the USPS test dataset\n",
    "        testset = torchvision.datasets.USPS(root=path, train=False,\n",
    "                                        download=True, transform=transform)\n",
    "    else:\n",
    "        raise ValueError('Invalid dataset. Please choose one of the supported datasets.')\n",
    "\n",
    "    # Create dataloaders for the training and test datasets\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                            shuffle=True, num_workers=num_workers)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                            shuffle=False, num_workers=num_workers)\n",
    "    \n",
    "    # Return the trainloader and testloader\n",
    "    return trainloader, testloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "297da6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_time_list = []\n",
    "testing_time_list = []\n",
    "accuracy_test_list = []\n",
    "loss_test_list = []\n",
    "epochs = range(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d89ffbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().to(device)  # Instancia o modelo e o move para a GPU, se disponível\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)  # Mova a função de perda para a GPU, se disponível\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)  # Define o otimizador para atualizar os parâmetros do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c772efd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch-summary] Model Summary with torchvision.datasets.MNIST\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 6, 24, 24]             156\n",
      "         MaxPool2d-2            [-1, 6, 12, 12]               0\n",
      "            Conv2d-3             [-1, 16, 8, 8]           2,416\n",
      "         MaxPool2d-4             [-1, 16, 4, 4]               0\n",
      "            Linear-5                  [-1, 120]          30,840\n",
      "            Linear-6                   [-1, 84]          10,164\n",
      "            Linear-7                   [-1, 10]             850\n",
      "================================================================\n",
      "Total params: 44,426\n",
      "Trainable params: 44,426\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.04\n",
      "Params size (MB): 0.17\n",
      "Estimated Total Size (MB): 0.22\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('[torch-summary] Model Summary with torchvision.datasets.MNIST')\n",
    "summary(net, (1, 28, 28))  # Resume o modelo, fornecendo o tamanho de entrada (1 canal, 28x28 pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92deec2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9912422/9912422 [00:00<00:00, 11489243.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28881/28881 [00:00<00:00, 17956669.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1648877/1648877 [00:00<00:00, 7796682.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4542/4542 [00:00<00:00, 7207918.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Starting training with 60000 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 1.305\n",
      "[1,   200] loss: 0.478\n",
      "[1,   300] loss: 0.296\n",
      "[1,   400] loss: 0.229\n",
      "[1,   500] loss: 0.159\n",
      "[1,   600] loss: 0.164\n",
      "[1,   700] loss: 0.125\n",
      "[1,   800] loss: 0.131\n",
      "[1,   900] loss: 0.122\n",
      "[1,  1000] loss: 0.101\n",
      "[1,  1100] loss: 0.119\n",
      "[1,  1200] loss: 0.119\n",
      "[1,  1300] loss: 0.110\n",
      "[1,  1400] loss: 0.089\n",
      "[1,  1500] loss: 0.090\n",
      "[1,  1600] loss: 0.085\n",
      "[1,  1700] loss: 0.073\n",
      "[1,  1800] loss: 0.079\n",
      "[1,  1875] Last loss: 0.081\n",
      "[2,   100] loss: 0.073\n",
      "[2,   200] loss: 0.078\n",
      "[2,   300] loss: 0.064\n",
      "[2,   400] loss: 0.067\n",
      "[2,   500] loss: 0.070\n",
      "[2,   600] loss: 0.057\n",
      "[2,   700] loss: 0.067\n",
      "[2,   800] loss: 0.072\n",
      "[2,   900] loss: 0.066\n",
      "[2,  1000] loss: 0.062\n",
      "[2,  1100] loss: 0.068\n",
      "[2,  1200] loss: 0.061\n",
      "[2,  1300] loss: 0.074\n",
      "[2,  1400] loss: 0.046\n",
      "[2,  1500] loss: 0.058\n",
      "[2,  1600] loss: 0.065\n",
      "[2,  1700] loss: 0.063\n",
      "[2,  1800] loss: 0.071\n",
      "[2,  1875] Last loss: 0.057\n",
      "Finished Training\n",
      "\n",
      "Training Time: 00:00:26\n",
      "Training Time Duration:  26.493074655532837\n",
      "\n",
      "Testing Time: 00:00:02\n",
      "Testing Time Duration:  2.2551076412200928\n",
      "\n",
      "Accuracy of the network on the 10000 test images: 0.976\n",
      "Loss of the network on the 10000 test images: 0.025\n",
      "\n",
      "Accuracy of the network on the 10000 test images: 97.550 %\n",
      "Loss of the network on the 10000 test images: 2.450 %\n"
     ]
    }
   ],
   "source": [
    "trainloader, testloader = load_dataset()  # Carrega os conjuntos de dados de treinamento e teste\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "print('Starting training with %d images' % ( len(trainloader.dataset)))\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in epochs:\n",
    "    running_loss = 0.0  # Variável para armazenar a perda acumulada durante o treinamento\n",
    "    \n",
    "   \n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Mova os dados para a GPU, se disponível\n",
    "\n",
    "        optimizer.zero_grad()  # Zera os gradientes dos parâmetros\n",
    "\n",
    "        outputs = net(inputs)  # Propagação direta dos dados através do modelo\n",
    "        loss = criterion(outputs, labels)  # Calcula a perda\n",
    "\n",
    "        loss.backward()  # Retropropagação para calcular os gradientes\n",
    "        optimizer.step()  # Atualiza os parâmetros do modelo com base nos gradientes\n",
    "\n",
    "        running_loss += loss.item()  # Acumula a perda para fins de exibição\n",
    "            \n",
    "        if i == len(trainloader) - 1:\n",
    "            last_loss = running_loss /  ((i % 100)+1)\n",
    "            print('[%d, %5d] Last loss: %.3f' % (epoch + 1, i+1, last_loss  ) )\n",
    "            loss_list.append(last_loss )  # Armazena o loss médio da época na lista\n",
    "\n",
    "        if i % 100 == 99:\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0       \n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "training_time_list.append(duration)\n",
    "# Converte a duração para o formato hh:mm:ss\n",
    "hours = int(duration // 3600)\n",
    "minutes = int((duration % 3600) // 60)\n",
    "seconds = int(duration % 60)\n",
    "\n",
    "print('Finished Training')\n",
    "print(f\"\\nTraining Time: {hours:02d}:{minutes:02d}:{seconds:02d}\")\n",
    "print('Training Time Duration: ',duration)\n",
    "\n",
    "\n",
    "correct = 0  # Variável para armazenar o número de previsões corretas\n",
    "total = 0  # Variável para armazenar o número total de exemplos de teste\n",
    "\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)  # Mova os dados para a GPU, se disponível\n",
    "\n",
    "        outputs = net(images)  # Propagação direta dos dados de teste através do modelo\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Obtém as previsões com maior probabilidade\n",
    "\n",
    "        total += labels.size(0)  # Atualiza o número total de exemplos de teste\n",
    "        correct += (predicted == labels).sum().item()  # Conta o número de previsões corretas\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "testing_time_list.append(duration)        \n",
    "        \n",
    "accuracy_test = correct / total\n",
    "loss_test = (total - correct) / total\n",
    "        \n",
    "accuracy_test_pct = 100 * accuracy_test\n",
    "loss_test_pct = 100 * loss_test\n",
    "\n",
    "accuracy_test_list.append(accuracy_test)\n",
    "loss_test_list.append(loss_test)\n",
    "\n",
    "# Converte a duração para o formato hh:mm:ss\n",
    "hours = int(duration // 3600)\n",
    "minutes = int((duration % 3600) // 60)\n",
    "seconds = int(duration % 60)\n",
    "print(f\"\\nTesting Time: {hours:02d}:{minutes:02d}:{seconds:02d}\")\n",
    "print('Testing Time Duration: ',duration)\n",
    "\n",
    "print('\\nAccuracy of the network on the %d test images: %.3f' % (len(testloader.dataset), accuracy_test))\n",
    "print('Loss of the network on the %d test images: %.3f' % (len(testloader.dataset), loss_test))\n",
    "print('\\nAccuracy of the network on the %d test images: %.3f %%' % (len(testloader.dataset), accuracy_test_pct))\n",
    "print('Loss of the network on the %d test images: %.3f %%' % (total, loss_test_pct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0b720cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(dataset='MNIST'):\n",
    "    trainloader, testloader = load_dataset(dataset)  # Load the training and test datasets\n",
    "\n",
    "    loss_list = []\n",
    "\n",
    "    print('[%s] Starting training with %d images' % (dataset, len(trainloader.dataset)))\n",
    "\n",
    "    start_time = time.time()\n",
    "    for epoch in epochs:\n",
    "        running_loss = 0.0  # Variable to store the accumulated loss during training\n",
    "\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move the data to the GPU, if available\n",
    "\n",
    "            optimizer.zero_grad()  # Zero the gradients of the parameters\n",
    "\n",
    "            outputs = net(inputs)  # Forward pass the data through the model\n",
    "            loss = criterion(outputs, labels)  # Calculate the loss\n",
    "\n",
    "            loss.backward()  # Backpropagation to compute the gradients\n",
    "            optimizer.step()  # Update the model parameters based on the gradients\n",
    "\n",
    "            running_loss += loss.item()  # Accumulate the loss for display purposes\n",
    "\n",
    "            if i == len(trainloader) - 1:\n",
    "                last_loss = running_loss / ((i % 100) + 1)\n",
    "                print('[%d, %5d] Last loss: %.3f' % (epoch + 1, i+1, last_loss))\n",
    "                loss_list.append(last_loss)  # Store the epoch's average loss in the list\n",
    "\n",
    "            if i % 100 == 99:\n",
    "                print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    print('[%s] Finished Training' % (dataset))\n",
    "    \n",
    "    \n",
    "    # Convert the duration to the format hh:mm:ss\n",
    "    #hours = int(training_time // 3600)\n",
    "    #minutes = int((training_time % 3600) // 60)\n",
    "    #seconds = int(training_time % 60)\n",
    "\n",
    "    #print('Finished Training')\n",
    "    #print(f\"\\nTraining Time: {hours:02d}:{minutes:02d}:{seconds:02d}\")\n",
    "    #print('Training Time Duration: ', training_time)\n",
    "\n",
    "    correct = 0  # Variable to store the number of correct predictions\n",
    "    total = 0  # Variable to store the total number of test examples\n",
    "\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)  # Move the data to the GPU, if available\n",
    "\n",
    "            outputs = net(images)  # Forward pass the test data through the model\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Get the predictions with highest probability\n",
    "\n",
    "            total += labels.size(0)  # Update the total number of test examples\n",
    "            correct += (predicted == labels).sum().item()  # Count the number of correct predictions\n",
    "\n",
    "    end_time = time.time()\n",
    "    testing_time = end_time - start_time\n",
    "\n",
    "    accuracy_test = correct / total\n",
    "    loss_test = (total - correct) / total\n",
    "\n",
    "    accuracy_test_pct = 100 * accuracy_test\n",
    "    loss_test_pct = 100 * loss_test\n",
    "\n",
    "    accuracy_test_list.append(accuracy_test)\n",
    "    loss_test_list.append(loss_test)\n",
    "\n",
    "\n",
    "    #print('\\nAccuracy of the network on the %d test images: %.3f' % (len(testloader.dataset), accuracy_test))\n",
    "    #print('Loss of the network on the %d test images: %.3f' % (len(testloader.dataset), loss_test))\n",
    "    #print('\\nAccuracy of the network on the %d test images: %.3f %%' % (len(testloader.dataset), accuracy_test_pct))\n",
    "    #print('Loss of the network on the %d test images: %.3f %%' % (total, loss_test_pct))\n",
    "    \n",
    "    return {\n",
    "        'loss_list': loss_list,\n",
    "        'training_time': training_time,\n",
    "        'testing_time': testing_time,\n",
    "        'accuracy_test': accuracy_test,\n",
    "        'loss_test': loss_test\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a10cc787",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_and_test_tqdm(dataset='MNIST'):\n",
    "    trainloader, testloader = load_dataset(dataset)  # Load the training and test datasets\n",
    "\n",
    "    loss_list = []\n",
    "    \n",
    "    print('[%s] %d training images / %d testing images' % (dataset, len(trainloader.dataset),len(testloader.dataset)))\n",
    "    \n",
    "\n",
    "    start_time = time.time()\n",
    "    for epoch in epochs:\n",
    "        running_loss = 0.0  # Variable to store the accumulated loss during training\n",
    "\n",
    "        # Use tqdm to create a progress bar for the training loop\n",
    "        with tqdm(total=len(trainloader), unit='batch', ncols=100, desc=\"Training\") as pbar:\n",
    "            for i, data in enumerate(trainloader, 0):\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)  # Move the data to the GPU, if available\n",
    "\n",
    "                optimizer.zero_grad()  # Zero the gradients of the parameters\n",
    "\n",
    "                outputs = net(inputs)  # Forward pass the data through the model\n",
    "                loss = criterion(outputs, labels)  # Calculate the loss\n",
    "\n",
    "                loss.backward()  # Backpropagation to compute the gradients\n",
    "                optimizer.step()  # Update the model parameters based on the gradients\n",
    "\n",
    "                running_loss += loss.item()  # Accumulate the loss for display purposes\n",
    "\n",
    "                if i == len(trainloader) - 1:\n",
    "                    last_loss = running_loss / ((i % 100) + 1)\n",
    "                    #print('[%d, %5d] Last loss: %.3f' % (epoch + 1, i+1, last_loss))\n",
    "                    loss_list.append(last_loss)  # Store the epoch's average loss in the list\n",
    "\n",
    "                if i % 100 == 99:\n",
    "                    #print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))\n",
    "                    running_loss = 0.0\n",
    "\n",
    "                # Update the tqdm progress bar\n",
    "                pbar.update(1)\n",
    "\n",
    "        end_time = time.time()\n",
    "        training_time = end_time - start_time\n",
    "\n",
    "        correct = 0  # Variable to store the number of correct predictions\n",
    "        total = 0  # Variable to store the total number of test examples\n",
    "\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            for data in testloader:\n",
    "                images, labels = data\n",
    "                images, labels = images.to(device), labels.to(device)  # Move the data to the GPU, if available\n",
    "\n",
    "                outputs = net(images)  # Forward pass the test data through the model\n",
    "                _, predicted = torch.max(outputs.data, 1)  # Get the predictions with highest probability\n",
    "\n",
    "                total += labels.size(0)  # Update the total number of test examples\n",
    "                correct += (predicted == labels).sum().item()  # Count the number of correct predictions\n",
    "\n",
    "        end_time = time.time()\n",
    "        testing_time = end_time - start_time\n",
    "\n",
    "        accuracy_test = correct / total\n",
    "        loss_test = (total - correct) / total\n",
    "\n",
    "        accuracy_test_pct = 100 * accuracy_test\n",
    "        loss_test_pct = 100 * loss_test\n",
    "\n",
    "        accuracy_test_list.append(accuracy_test)\n",
    "        loss_test_list.append(loss_test)\n",
    "\n",
    "        return {\n",
    "            'loss_list': loss_list,\n",
    "            'training_time': training_time,\n",
    "            'testing_time': testing_time,\n",
    "            'accuracy_test': accuracy_test,\n",
    "            'loss_test': loss_test\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a36f8beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MNIST] 60000 training images / 10000 testing images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████| 1875/1875 [00:13<00:00, 138.89batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FashionMNIST] 60000 training images / 10000 testing images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|█████████████████████████████████████████████| 1875/1875 [00:13<00:00, 139.10batch/s]\n"
     ]
    }
   ],
   "source": [
    "result_MNIST = train_and_test_tqdm()\n",
    "result_FashionMNIST = train_and_test_tqdm('FashionMNIST')\n",
    "#result_EMNIST = train_and_test('EMNIST')\n",
    "#result_KMNIST = train_and_test('KMNIST')\n",
    "#result_QMNIST = train_and_test('QMNIST')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "619d3fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Agora você pode usar os valores retornados da seguinte maneira\n",
      "Loss list: [0.01681104075919703, 0.007888377564149777]\n",
      "Training time: 26.64787006378174\n",
      "Testing time: 2.280391216278076\n",
      "Accuracy on test images: 0.9912\n",
      "Loss on test images: 0.0088\n"
     ]
    }
   ],
   "source": [
    "loss_list = result_MNIST['loss_list']\n",
    "training_time = result_MNIST['training_time']\n",
    "testing_time = result_MNIST['testing_time']\n",
    "accuracy_test = result_MNIST['accuracy_test']\n",
    "loss_test = result_MNIST['loss_test']\n",
    "\n",
    "print('\\n\\nAgora você pode usar os valores retornados da seguinte maneira')\n",
    "print('Loss list:', loss_list)\n",
    "print('Training time:', training_time)\n",
    "print('Testing time:', testing_time)\n",
    "print('Accuracy on test images:', accuracy_test)\n",
    "print('Loss on test images:', loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c4e4ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ef3e54ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9904"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_test_MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "28427135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss_NMIST</th>\n",
       "      <th>loss_FashionMNIST</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.016811</td>\n",
       "      <td>0.338154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.007888</td>\n",
       "      <td>0.293769</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       loss_NMIST  loss_FashionMNIST\n",
       "epoch                               \n",
       "1        0.016811           0.338154\n",
       "2        0.007888           0.293769"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_list = [x + 1 for x in list(epochs)]\n",
    "\n",
    "table_data = {'epoch': epoch_list, \n",
    "              'loss_NMIST': result_MNIST['loss_list'], \n",
    "              'loss_FashionMNIST': result_FashionMNIST['loss_list']}\n",
    "df_training = pd.DataFrame(table_data)\n",
    "df_training.set_index('epoch', inplace=True)\n",
    "df_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72bf60ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Dataset</th>\n",
       "      <th>NMIST</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Training Time (s)</th>\n",
       "      <td>26.493075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Testing Time (s)</th>\n",
       "      <td>2.255108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy Test</th>\n",
       "      <td>0.975500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loss Test</th>\n",
       "      <td>0.024500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Dataset                NMIST\n",
       "Training Time (s)  26.493075\n",
       "Testing Time (s)    2.255108\n",
       "Accuracy Test       0.975500\n",
       "Loss Test           0.024500"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually create the dataset_list\n",
    "dataset_list = [\"NMIST\"]\n",
    "#dataset_list = [\"NMIST\",\"CIFAR10\",\"CIFAR100\",\"FashionMNIST\",\"EMNIST\",\"KMNIST\",\"QMNIST\",\"STL10\",\"USPS\"] \n",
    "\n",
    "# Create a dictionary with the list values\n",
    "data = {'Dataset': dataset_list,\n",
    "        'Training Time (s)': training_time_list,\n",
    "        'Testing Time (s)': testing_time_list,\n",
    "        'Accuracy Test': accuracy_test_list,\n",
    "        'Loss Test': loss_test_list}\n",
    "\n",
    "# Create a DataFrame from the dictionary\n",
    "df_model_dataset = pd.DataFrame(data)\n",
    "\n",
    "df_model_dataset.set_index('Dataset', inplace=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "df_model_dataset.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a96c2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
